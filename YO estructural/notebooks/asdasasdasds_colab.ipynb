{"cells":[{"cell_type":"markdown","metadata":{"id":"smuxtZybo5BX"},"source":["# Celda 1: T√≠tulo y Descripci√≥n (Markdown)\n","# üß† Sistema Fenomenol√≥gico Estructural v2.3 - Colab Edition\n","# Este notebook procesa textos fenomenol√≥gicos, gestiona un modelo sem√°ntico y genera an√°lisis detallados usando el sistema YO emergente.\n","# Compatible con Google Colab.\n","#\n","# Instrucciones:\n","# 1. Ejecuta cada celda en orden.\n","# 2. Sube tus archivos de texto de entrada a la carpeta `/content/YO estructural/entrada_bruta/` despu√©s de ejecutar la celda de creaci√≥n de directorios.\n","# 3. Revisa los archivos de salida en `/content/YO estructural/logs_sistema/` y otras carpetas seg√∫n la configuraci√≥n.\n"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5553,"status":"ok","timestamp":1749989992405,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"},"user_tz":360},"id":"v-y35e7no5Bf","outputId":"52f1d848-36ba-4268-f858-953647ffe798"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}],"source":["# Celda 2: Instalaci√≥n de Dependencias\n","!pip install pandas numpy scikit-learn pyyaml networkx"]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137,"status":"ok","timestamp":1749989992545,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"},"user_tz":360},"id":"723rqnIIo5Bi","outputId":"499c552f-8b6d-445f-cf31-7cebbe1c23d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Directorio base '/content/YO estructural' y subdirectorios creados.\n","Archivo '/content/YO estructural/configuracion/config.yaml' creado.\n","Archivo 'scripts/__init__.py' creado.\n","Archivo 'scripts/modelos/__init__.py' creado.\n","Archivo 'scripts/modelos/fenomeno.py' creado.\n","Archivo 'scripts/modelos/contexto.py' creado.\n","Archivo 'scripts/modelos/macrocontexto.py' creado.\n","Archivo 'scripts/modelos/red_contextos.py' creado.\n","Archivo 'scripts/modelos/ontosistema.py' creado.\n","Archivo 'scripts/modelos/metacampo.py' creado.\n","Archivo 'scripts/extractor_features.py' creado.\n","Archivo 'scripts/clasificador.py' creado.\n","Archivo 'scripts/gestor_modelo_semantico.py' creado.\n","Archivo 'scripts/analizador_sistema.py' creado.\n","Archivos de texto de ejemplo creados/renombrados en 'entrada_bruta' para generar m√∫ltiples etiquetas.\n","\n","--- Configuraci√≥n de archivos completa. Puede proceder a la siguiente celda. ---\n"]}],"source":["# Celda 3: Creaci√≥n de Estructura de Directorios y Archivos Esenciales\n","import os\n","import yaml\n","\n","# --- Directorio Ra√≠z del Proyecto en Colab ---\n","BASE_DIR = '/content/YO estructural'\n","DIRECTORIES = [\n","    'configuracion',\n","    'entrada_bruta',\n","    'features_extraidas',\n","    'clasificados',\n","    'logs_sistema',\n","    'scripts',\n","    'scripts/modelos',\n","    'modelo_semantico/fenomenos',\n","    'modelo_semantico/contextos',\n","    'modelo_semantico/macrocontextos',\n","    'modelo_semantico/redes',\n","    'modelo_semantico/conceptos_emergentes',\n","    'modelo_semantico/metacampos'\n","]\n","\n","for dirname in DIRECTORIES:\n","    os.makedirs(os.path.join(BASE_DIR, dirname), exist_ok=True)\n","\n","print(f\"Directorio base '{BASE_DIR}' y subdirectorios creados.\")\n","\n","# --- Creaci√≥n de config.yaml ---\n","config_content = {\n","    'procesamiento': {\n","        'idioma': 'es',\n","        'longitud_minima_texto': 10,\n","        'tfidf_max_features': 5000,\n","        'tfidf_ngram_range': [1, 2] # Changed from tuple (1, 2) to list [1, 2] for YAML compatibility\n","    },\n","    'clasificacion': {\n","        'algoritmo': 'RandomForest',\n","        'n_estimators': 100,\n","        'test_size': 0.25, # Ajustar si hay pocos datos. M√≠nimo 2 muestras por clase.\n","        'random_state': 42\n","    },\n","    'fenomenologia': {\n","        'categorias_principales': ['percepcion', 'emocion', 'cognicion', 'relacional', 'metafisico'],\n","        'umbral_relevancia_termino': 0.1,\n","        'generar_nodos_obsidian': False\n","    },\n","    'sistema': {\n","        'backup_automatico': True,\n","        'intervalo_backup_horas': 24,\n","        'generar_reporte_analisis': True\n","    },\n","    'modelo_semantico': {\n","        'rutas': {\n","            'fenomenos': 'modelo_semantico/fenomenos',\n","            'contextos': 'modelo_semantico/contextos',\n","            'macrocontextos': 'modelo_semantico/macrocontextos',\n","            'redes': 'modelo_semantico/redes',\n","            'conceptos_emergentes': 'modelo_semantico/conceptos_emergentes',\n","            'metacampos': 'modelo_semantico/metacampos'\n","        },\n","        'umbral_agrupamiento_similaridad': 0.7,\n","        'umbral_emergencia_concepto': 0.8,\n","        'umbral_creacion_metacampo': 3,\n","        'neo4j': {\n","            'activado': False,\n","            'uri': 'bolt://localhost:7687',\n","            'usuario': 'neo4j',\n","            'contrasena': 'password'\n","        }\n","    }\n","}\n","\n","CONFIG_PATH = os.path.join(BASE_DIR, 'configuracion', 'config.yaml')\n","with open(CONFIG_PATH, 'w', encoding='utf-8') as f:\n","    yaml.dump(config_content, f, default_flow_style=False, allow_unicode=True)\n","print(f\"Archivo '{CONFIG_PATH}' creado.\")\n","\n","# --- Creaci√≥n de scripts/__init__.py ---\n","scripts_init_content = \"\"\"\n","# This file makes the 'scripts' directory a Python package.\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', '__init__.py'), 'w', encoding='utf-8') as f:\n","    f.write(scripts_init_content)\n","print(\"Archivo 'scripts/__init__.py' creado.\")\n","\n","\n","# --- Creaci√≥n de scripts/modelos/__init__.py ---\n","modelos_init_content = \"\"\"\n","# Archivo de inicializaci√≥n del paquete modelos\n","from .fenomeno import Fenomeno\n","from .contexto import Contexto\n","from .macrocontexto import Macrocontexto\n","from .red_contextos import RedContextos\n","from .ontosistema import Ontosistema\n","from .metacampo import Metacampo\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'modelos', '__init__.py'), 'w', encoding='utf-8') as f:\n","    f.write(modelos_init_content)\n","print(\"Archivo 'scripts/modelos/__init__.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/modelos/fenomeno.py ---\n","fenomeno_content = \"\"\"\n","import uuid\n","import datetime\n","\n","class Fenomeno:\n","    def __init__(self, descripcion=\"\", tipo=\"\", intensidad=0.0, polaridad=\"neutra\"):\n","        self.id = str(uuid.uuid4())[:8]\n","        self.descripcion = descripcion\n","        self.tipo = tipo # Ej: 'visual', 'auditivo', 'pensamiento'\n","        self.intensidad = intensidad # Escala 0-1\n","        self.polaridad = polaridad # Ej: 'positivo', 'negativo', 'neutra'\n","        self.conceptos_asociados = [] # Lista de strings (palabras clave)\n","        self.timestamp = datetime.datetime.now().isoformat()\n","\n","    def to_dict(self):\n","        return self.__dict__\n","\n","    @classmethod\n","    def from_dict(cls, data):\n","        fenomeno = cls()\n","        fenomeno.__dict__.update(data)\n","        return fenomeno\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'modelos', 'fenomeno.py'), 'w', encoding='utf-8') as f:\n","    f.write(fenomeno_content)\n","print(\"Archivo 'scripts/modelos/fenomeno.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/modelos/contexto.py ---\n","contexto_content = \"\"\"\n","import uuid\n","import datetime\n","\n","class Contexto:\n","    def __init__(self, nombre=\"\", descripcion=\"\"):\n","        self.id = str(uuid.uuid4())[:8]\n","        self.nombre = nombre\n","        self.descripcion = descripcion\n","        self.fenomenos_ids = [] # Lista de IDs de Fenomenos\n","        self.macrocontexto_id = None\n","        self.relaciones = {} # {id_otro_contexto: tipo_relacion}\n","        self.timestamp = datetime.datetime.now().isoformat()\n","        self.yo_presente = False # Indicador de la presencia del 'YO'\n","\n","    def agregar_fenomeno(self, fenomeno_id):\n","        if fenomeno_id not in self.fenomenos_ids:\n","            self.fenomenos_ids.append(fenomeno_id)\n","\n","    def to_dict(self):\n","        return self.__dict__\n","\n","    @classmethod\n","    def from_dict(cls, data):\n","        contexto = cls()\n","        contexto.__dict__.update(data)\n","        return contexto\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'modelos', 'contexto.py'), 'w', encoding='utf-8') as f:\n","    f.write(contexto_content)\n","print(\"Archivo 'scripts/modelos/contexto.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/modelos/macrocontexto.py ---\n","macrocontexto_content = \"\"\"\n","import uuid\n","import datetime\n","\n","class Macrocontexto:\n","    def __init__(self, nombre=\"\", descripcion=\"\"):\n","        self.id = str(uuid.uuid4())[:8]\n","        self.nombre = nombre\n","        self.descripcion = descripcion\n","        self.contextos_ids = [] # Lista de IDs de Contextos\n","        self.temas_centrales = [] # Lista de strings\n","        self.timestamp = datetime.datetime.now().isoformat()\n","\n","    def agregar_contexto(self, contexto_id):\n","        if contexto_id not in self.contextos_ids:\n","            self.contextos_ids.append(contexto_id)\n","\n","    def to_dict(self):\n","        return self.__dict__\n","\n","    @classmethod\n","    def from_dict(cls, data):\n","        mc = cls()\n","        mc.__dict__.update(data)\n","        return mc\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'modelos', 'macrocontexto.py'), 'w', encoding='utf-8') as f:\n","    f.write(macrocontexto_content)\n","print(\"Archivo 'scripts/modelos/macrocontexto.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/modelos/red_contextos.py ---\n","red_contextos_content = \"\"\"\n","import uuid\n","import datetime\n","import networkx as nx # Necesita ser instalado\n","import yaml\n","import os\n","\n","class RedContextos:\n","    def __init__(self, nombre=\"\", descripcion=\"\"):\n","        self.id = str(uuid.uuid4())[:8]\n","        self.nombre = nombre\n","        self.descripcion = descripcion\n","        self.grafo = nx.DiGraph()\n","        self.timestamp = datetime.datetime.now().isoformat()\n","        self.propiedades_emergentes = {}\n","\n","    def agregar_nodo(self, id_nodo, tipo=\"contexto\", atributos=None):\n","        self.grafo.add_node(id_nodo, tipo=tipo, **(atributos or {}))\n","\n","    def agregar_relacion(self, id_origen, id_destino, tipo=\"relacionado_con\", peso=1.0, atributos=None):\n","        self.grafo.add_edge(id_origen, id_destino, tipo=tipo, peso=peso, **(atributos or {}))\n","\n","    def to_dict(self):\n","        nodos = [{'id': n, **self.grafo.nodes[n]} for n in self.grafo.nodes()]\n","        relaciones = [{'origen': u, 'destino': v, **self.grafo.edges[u,v]} for u,v in self.grafo.edges()]\n","        return {\n","            'id': self.id,\n","            'nombre': self.nombre,\n","            'descripcion': self.descripcion,\n","            'nodos': nodos,\n","            'relaciones': relaciones,\n","            'propiedades_emergentes': self.propiedades_emergentes,\n","            'timestamp': self.timestamp\n","        }\n","\n","    @classmethod\n","    def from_dict(cls, data):\n","        red = cls(data['nombre'], data['descripcion'])\n","        red.id = data['id']\n","        red.timestamp = data.get('timestamp', datetime.datetime.now().isoformat())\n","        red.propiedades_emergentes = data.get('propiedades_emergentes', {})\n","        for nodo_data in data.get('nodos', []):\n","            nodo_id = nodo_data.pop('id')\n","            red.grafo.add_node(nodo_id, **nodo_data)\n","        for rel_data in data.get('relaciones', []):\n","            origen = rel_data.pop('origen')\n","            destino = rel_data.pop('destino')\n","            red.grafo.add_edge(origen, destino, **rel_data)\n","        return red\n","\n","    def guardar(self, ruta_base_modelo_semantico):\n","        ruta_redes = os.path.join(ruta_base_modelo_semantico, 'redes')\n","        os.makedirs(ruta_redes, exist_ok=True)\n","        ruta_archivo = os.path.join(ruta_redes, f\"red_{self.id}.yaml\")\n","        with open(ruta_archivo, 'w', encoding='utf-8') as f:\n","            yaml.dump(self.to_dict(), f, default_flow_style=False, allow_unicode=True)\n","        return ruta_archivo\n","\n","    @classmethod\n","    def cargar(cls, ruta_archivo):\n","        with open(ruta_archivo, 'r', encoding='utf-8') as f:\n","            data = yaml.safe_load(f)\n","        return cls.from_dict(data)\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'modelos', 'red_contextos.py'), 'w', encoding='utf-8') as f:\n","    f.write(red_contextos_content)\n","print(\"Archivo 'scripts/modelos/red_contextos.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/modelos/ontosistema.py ---\n","ontosistema_content = \"\"\"\n","import os\n","import yaml\n","import datetime\n","import json\n","\n","class Ontosistema:\n","    def __init__(self, config, base_dir):\n","        self.config = config\n","        self.base_dir = base_dir # Directorio ra√≠z del proyecto '/content/YO estructural'\n","        self.rutas_relativas = config.get('modelo_semantico', {}).get('rutas', {})\n","        self.estadisticas = {\n","            \"fenomenos\": 0,\n","            \"contextos\": 0,\n","            \"macrocontextos\": 0,\n","            \"redes\": 0,\n","            \"conceptos_emergentes\": 0,\n","            \"metacampos\": 0,\n","            \"apariciones_yo\": 0\n","        }\n","        self.ultima_actualizacion = datetime.datetime.now().isoformat()\n","\n","    def _get_ruta_absoluta(self, tipo_entidad):\n","        ruta_relativa = self.rutas_relativas.get(tipo_entidad)\n","        if ruta_relativa:\n","            return os.path.join(self.base_dir, ruta_relativa)\n","        return None\n","\n","    def actualizar_estadisticas(self):\n","        for tipo, ruta_rel in self.rutas_relativas.items():\n","            ruta_completa = os.path.join(self.base_dir, ruta_rel)\n","            if os.path.exists(ruta_completa) and os.path.isdir(ruta_completa):\n","                archivos = [f for f in os.listdir(ruta_completa) if f.endswith('.yaml')]\n","                self.estadisticas[tipo] = len(archivos)\n","            else:\n","                self.estadisticas[tipo] = 0\n","        self.contar_apariciones_yo()\n","        self.ultima_actualizacion = datetime.datetime.now().isoformat()\n","        return self.estadisticas\n","\n","    def contar_apariciones_yo(self):\n","        ruta_contextos_abs = self._get_ruta_absoluta('contextos')\n","        if ruta_contextos_abs and os.path.exists(ruta_contextos_abs):\n","            contador = 0\n","            for archivo in os.listdir(ruta_contextos_abs):\n","                if archivo.endswith('.yaml'):\n","                    try:\n","                        with open(os.path.join(ruta_contextos_abs, archivo), 'r', encoding='utf-8') as f:\n","                            data = yaml.safe_load(f)\n","                            if data.get('yo_presente', False):\n","                                contador += 1\n","                    except Exception as e:\n","                        print(f\"Error al leer {archivo}: {e}\")\n","            self.estadisticas[\"apariciones_yo\"] = contador\n","        else:\n","            self.estadisticas[\"apariciones_yo\"] = 0\n","\n","    def guardar_estadisticas(self):\n","        ruta_logs = os.path.join(self.base_dir, 'logs_sistema')\n","        os.makedirs(ruta_logs, exist_ok=True)\n","        ruta_completa = os.path.join(ruta_logs, 'metricas_ontosistema.json')\n","        datos = {\n","            \"estadisticas\": self.estadisticas,\n","            \"timestamp\": self.ultima_actualizacion\n","        }\n","        with open(ruta_completa, 'w', encoding='utf-8') as f:\n","            json.dump(datos, f, indent=2, ensure_ascii=False)\n","        return ruta_completa\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'modelos', 'ontosistema.py'), 'w', encoding='utf-8') as f:\n","    f.write(ontosistema_content)\n","print(\"Archivo 'scripts/modelos/ontosistema.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/modelos/metacampo.py ---\n","metacampo_content = \"\"\"\n","import uuid\n","import datetime\n","\n","class Metacampo:\n","    def __init__(self, nombre=\"\", descripcion=\"\"):\n","        self.id = str(uuid.uuid4())[:8]\n","        self.nombre = nombre\n","        self.descripcion = descripcion\n","        self.conceptos_clave = [] # Lista de strings\n","        self.entidades_relacionadas_ids = [] # IDs de fenomenos, contextos, etc.\n","        self.timestamp = datetime.datetime.now().isoformat()\n","\n","    def to_dict(self):\n","        return self.__dict__\n","\n","    @classmethod\n","    def from_dict(cls, data):\n","        mc = cls()\n","        mc.__dict__.update(data)\n","        return mc\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'modelos', 'metacampo.py'), 'w', encoding='utf-8') as f:\n","    f.write(metacampo_content)\n","print(\"Archivo 'scripts/modelos/metacampo.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/extractor_features.py ---\n","extractor_content = \"\"\"\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import re\n","import os\n","\n","class ExtractorFeatures:\n","    def __init__(self, config):\n","        self.config_proc = config.get('procesamiento', {})\n","        self.vectorizer = TfidfVectorizer(\n","            max_features=self.config_proc.get('tfidf_max_features', 5000),\n","            ngram_range=tuple(self.config_proc.get('tfidf_ngram_range', [1, 2])), # Use list from config\n","            stop_words=self._get_stopwords()\n","        )\n","\n","    def _get_stopwords(self):\n","        # Placeholder for stopwords, customize for your language\n","        if self.config_proc.get('idioma', 'es') == 'es':\n","            return ['de', 'la', 'el', 'en', 'y', 'o', 'que', 'un', 'una', 'los', 'las', 'con', 'por', 'para']\n","        return None\n","\n","    def limpiar_texto(self, texto):\n","        texto = texto.lower()\n","        texto = re.sub(r'\\d+', '', texto) # Eliminar n√∫meros\n","        texto = re.sub(r'\\s+', ' ', texto).strip() # Eliminar espacios extra\n","        # A√±adir m√°s limpieza si es necesario\n","        return texto\n","\n","    def extraer_features_tfidf(self, textos_crudos):\n","        textos_limpios = [self.limpiar_texto(t) for t in textos_crudos]\n","        if not textos_limpios or all(not t for t in textos_limpios):\n","            print(\"Advertencia: No hay textos v√°lidos para extraer caracter√≠sticas despu√©s de la limpieza.\")\n","            return pd.DataFrame()\n","        features = self.vectorizer.fit_transform(textos_limpios)\n","        return pd.DataFrame(features.toarray(), columns=self.vectorizer.get_feature_names_out())\n","\n","    def guardar_features(self, df_features, etiquetas, base_dir):\n","        ruta_salida = os.path.join(base_dir, 'features_extraidas')\n","        os.makedirs(ruta_salida, exist_ok=True)\n","        df_features.to_csv(os.path.join(ruta_salida, 'features_tfidf.csv'), index=False)\n","        pd.DataFrame({'clase': etiquetas}).to_csv(os.path.join(ruta_salida, 'etiquetas.csv'), index=False)\n","        print(f\"Caracter√≠sticas y etiquetas guardadas en {ruta_salida}\")\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'extractor_features.py'), 'w', encoding='utf-8') as f:\n","    f.write(extractor_content)\n","print(\"Archivo 'scripts/extractor_features.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/clasificador.py ---\n","clasificador_content = \"\"\"\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import joblib\n","import os\n","import numpy as np\n","\n","class ClasificadorFenomenologico:\n","    def __init__(self, config):\n","        self.config_clas = config.get('clasificacion', {})\n","        self.modelo = RandomForestClassifier(\n","            n_estimators=self.config_clas.get('n_estimators', 100),\n","            random_state=self.config_clas.get('random_state', 42)\n","        )\n","        self.categorias = config.get('fenomenologia', {}).get('categorias_principales', [])\n","\n","    def entrenar(self, X, y):\n","        if X.shape[0] == 0:\n","            print(\"Error: No hay datos para entrenar.\")\n","            return False # Indicate failure\n","\n","        unique_classes = np.unique(y)\n","        if len(unique_classes) < 2:\n","            print(f\"Error: Se necesitan al menos 2 clases √∫nicas para entrenar. Se encontraron {len(unique_classes)}.\")\n","            return False # Indicate failure\n","\n","        # Asegurar que test_size sea apropiado\n","        test_size = self.config_clas.get('test_size', 0.25)\n","        if X.shape[0] * test_size < len(unique_classes) or X.shape[0] * (1-test_size) < len(unique_classes):\n","             print(f\"Advertencia: Pocos datos para el split con test_size={test_size}. Se usar√° el dataset completo para entrenar y evaluar sobre √©l mismo.\")\n","             self.modelo.fit(X,y)\n","             print(\"Modelo entrenado con todos los datos.\")\n","             y_pred = self.modelo.predict(X)\n","             print(f\"Classification Report (sobre datos de entrenamiento):\\\\n{classification_report(y, y_pred, zero_division=0)}\")\n","             return True # Indicate success\n","\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            X, y,\n","            test_size=test_size,\n","            random_state=self.config_clas.get('random_state', 42),\n","            stratify=y if len(unique_classes) > 1 and all(np.sum(y == label) > 1 for label in unique_classes) else None\n","        )\n","        self.modelo.fit(X_train, y_train)\n","        print(\"Modelo entrenado.\")\n","        y_pred = self.modelo.predict(X_test)\n","        print(f\"Classification Report:\\\\n{classification_report(y_test, y_test, zero_division=0)}\") # Corrected to use y_test for evaluation\n","        return True # Indicate success\n","\n","\n","    def predecir(self, X_nuevos):\n","        return self.modelo.predict(X_nuevos)\n","\n","    def predecir_probabilidades(self, X_nuevos):\n","        return self.modelo.predict_proba(X_nuevos)\n","\n","    def guardar_modelo(self, base_dir):\n","        ruta_salida = os.path.join(base_dir, 'clasificados')\n","        os.makedirs(ruta_salida, exist_ok=True)\n","        joblib.dump(self.modelo, os.path.join(ruta_salida, 'clasificador_fenomenologico.joblib'))\n","        print(f\"Modelo guardado en {ruta_salida}\")\n","\n","    def cargar_modelo(self, base_dir):\n","        ruta_modelo = os.path.join(base_dir, 'clasificados', 'clasificador_fenomenologico.joblib')\n","        if os.path.exists(ruta_modelo):\n","            self.modelo = joblib.load(ruta_modelo)\n","            print(\"Modelo cargado.\")\n","        else:\n","            print(\"No se encontr√≥ un modelo guardado.\")\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'clasificador.py'), 'w', encoding='utf-8') as f:\n","    f.write(clasificador_content)\n","print(\"Archivo 'scripts/clasificador.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/gestor_modelo_semantico.py ---\n","gestor_content = \"\"\"\n","import os\n","import yaml\n","import datetime\n","from .modelos import Fenomeno, Contexto, Macrocontexto, RedContextos, Ontosistema, Metacampo\n","\n","class GestorModeloSemantico:\n","    def __init__(self, config, base_dir):\n","        self.config = config\n","        self.base_dir = base_dir # Directorio ra√≠z del proyecto '/content/YO estructural'\n","        self.rutas_modelo = config.get('modelo_semantico', {}).get('rutas', {})\n","        self.ontosistema = Ontosistema(config, base_dir)\n","        self._crear_directorios_modelo()\n","\n","    def _get_ruta_absoluta_entidad(self, tipo_entidad):\n","        ruta_relativa = self.rutas_modelo.get(tipo_entidad)\n","        if ruta_relativa:\n","            return os.path.join(self.base_dir, ruta_relativa)\n","        raise ValueError(f\"Ruta no definida para la entidad: {tipo_entidad}\")\n","\n","    def _crear_directorios_modelo(self):\n","        for _, ruta_relativa in self.rutas_modelo.items():\n","            os.makedirs(os.path.join(self.base_dir, ruta_relativa), exist_ok=True)\n","        print(\"Directorios del modelo sem√°ntico verificados/creados.\")\n","\n","    def guardar_entidad(self, entidad, tipo_entidad):\n","        ruta_directorio_entidad = self._get_ruta_absoluta_entidad(tipo_entidad)\n","        ruta_archivo = os.path.join(ruta_directorio_entidad, f\"{tipo_entidad}_{entidad.id}.yaml\")\n","        with open(ruta_archivo, 'w', encoding='utf-8') as f:\n","            yaml.dump(entidad.to_dict(), f, default_flow_style=False, allow_unicode=True)\n","        print(f\"{tipo_entidad.capitalize()} '{entidad.id}' guardado en {ruta_archivo}\")\n","        self.ontosistema.actualizar_estadisticas()\n","        return ruta_archivo\n","\n","    def cargar_entidad(self, id_entidad, tipo_entidad):\n","        clase_entidad_map = {\n","            'fenomenos': Fenomeno,\n","            'contextos': Contexto,\n","            'macrocontextos': Macrocontexto,\n","            'redes': RedContextos,\n","            'metacampos': Metacampo\n","        }\n","        if tipo_entidad not in clase_entidad_map:\n","            raise ValueError(f\"Tipo de entidad desconocido: {tipo_entidad}\")\n","\n","        ClaseEntidad = clase_entidad_map[tipo_entidad]\n","        ruta_directorio_entidad = self._get_ruta_absoluta_entidad(tipo_entidad)\n","        ruta_archivo = os.path.join(ruta_directorio_entidad, f\"{tipo_entidad}_{id_entidad}.yaml\")\n","\n","        if not os.path.exists(ruta_archivo):\n","            print(f\"No se encontr√≥ el archivo para {tipo_entidad} con ID {id_entidad} en {ruta_archivo}\")\n","            return None\n","\n","        with open(ruta_archivo, 'r', encoding='utf-8') as f:\n","            data = yaml.safe_load(f)\n","        return ClaseEntidad.from_dict(data)\n","\n","    def crear_fenomeno(self, descripcion, tipo, intensidad, polaridad):\n","        fenomeno = Fenomeno(descripcion, tipo, intensidad, polaridad)\n","        self.guardar_entidad(fenomeno, 'fenomenos')\n","        return fenomeno\n","\n","    def crear_contexto(self, nombre, descripcion, yo_presente=False):\n","        contexto = Contexto(nombre, descripcion)\n","        contexto.yo_presente = yo_presente\n","        self.guardar_entidad(contexto, 'contextos')\n","        return contexto\n","\n","    def crear_macrocontexto(self, nombre, descripcion):\n","        mc = Macrocontexto(nombre, descripcion)\n","        self.guardar_entidad(mc, 'macrocontextos')\n","        return mc\n","\n","    def crear_red_contextos(self, nombre, descripcion):\n","        red = RedContextos(nombre, descripcion)\n","        # Las redes se guardan a trav√©s de su propio m√©todo guardar que ya conoce la ruta base\n","        red.guardar(os.path.join(self.base_dir, 'modelo_semantico')) # Pasar ruta base para 'redes'\n","        self.ontosistema.actualizar_estadisticas()\n","        return red\n","\n","    def inicializar_ontosistema(self):\n","        self.ontosistema.actualizar_estadisticas()\n","        self.ontosistema.guardar_estadisticas()\n","        print(\"Ontosistema inicializado y estad√≠sticas guardadas.\")\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'gestor_modelo_semantico.py'), 'w', encoding='utf-8') as f:\n","    f.write(gestor_content)\n","print(\"Archivo 'scripts/gestor_modelo_semantico.py' creado.\")\n","\n","# --- Creaci√≥n de scripts/analizador_sistema.py ---\n","analizador_content = \"\"\"\n","import os\n","import yaml\n","import pandas as pd\n","from collections import Counter\n","import json # Import json here\n","# No importar GestorModeloSemantico y ClasificadorFenomenologico aqu√≠ para evitar dependencia circular\n","# Se pasar√°n como argumentos o se instanciar√°n dentro de las funciones si es necesario.\n","\n","def cargar_config_analizador(config_path):\n","    if not os.path.exists(config_path):\n","        print(f\"Error: El archivo de configuraci√≥n no se encuentra en {config_path}\")\n","        return None\n","    with open(config_path, 'r', encoding='utf-8') as f:\n","        return yaml.safe_load(f)\n","\n","def analizar_sistema(base_dir, config_path_analizador):\n","    config = cargar_config_analizador(config_path_analizador)\n","    if not config:\n","        return \"Error: No se pudo cargar la configuraci√≥n.\"\n","\n","    reporte_path = os.path.join(base_dir, 'logs_sistema', 'reporte_analisis.txt')\n","\n","    with open(reporte_path, 'w', encoding='utf-8') as f:\n","        f.write(\"=== REPORTE DE AN√ÅLISIS DEL SISTEMA (COLAB) ===\\n\\n\")\n","\n","        # Verificar estructura de directorios\n","        directorios_check = ['entrada_bruta', 'features_extraidas', 'clasificados', 'logs_sistema', 'configuracion', 'modelo_semantico']\n","        f.write(\"1. Verificaci√≥n de directorios base:\\n\")\n","        for dir_check_name in directorios_check:\n","            path = os.path.join(base_dir, dir_check_name)\n","            existe = os.path.exists(path) and os.path.isdir(path)\n","            f.write(f\"   - {dir_check_name}: {'‚úì' if existe else 'X'}\\n\")\n","\n","        # Verificar archivos necesarios\n","        f.write(\"\\n2. Verificaci√≥n de archivos clave:\\n\")\n","        archivos_clave = {\n","            'config.yaml': os.path.join(base_dir, 'configuracion', 'config.yaml'),\n","            # Se verificar√°n features y etiquetas despu√©s de la extracci√≥n\n","        }\n","        for nombre, ruta_abs in archivos_clave.items():\n","            existe = os.path.exists(ruta_abs) and os.path.isfile(ruta_abs)\n","            f.write(f\"   - {nombre}: {'‚úì' if existe else 'X'}\\n\")\n","\n","        f.write(\"\\n3. An√°lisis de datos (despu√©s de ejecutar extractor_features):\\n\")\n","        path_features = os.path.join(base_dir, 'features_extraidas', 'features_tfidf.csv')\n","        path_etiquetas = os.path.join(base_dir, 'features_extraidas', 'etiquetas.csv')\n","\n","        if os.path.exists(path_features) and os.path.exists(path_etiquetas):\n","            try:\n","                df_features = pd.read_csv(path_features)\n","                df_etiquetas = pd.read_csv(path_etiquetas)\n","                y = df_etiquetas['clase'].tolist()\n","\n","                f.write(f\"   - N√∫mero de textos procesados (features): {len(df_features)}\\n\")\n","                f.write(f\"   - N√∫mero de features extra√≠das: {df_features.shape[1]}\\n\")\n","                f.write(f\"   - N√∫mero de etiquetas: {len(y)}\\n\")\n","                if y:\n","                    f.write(f\"   - Categor√≠as √∫nicas: {len(set(y))}\\n\")\n","                    conteo = Counter(y)\n","                    f.write(\"\\n4. Distribuci√≥n de categor√≠as:\\n\")\n","                    for categoria, count in conteo.items():\n","                        f.write(f\"   - {categoria}: {count} ejemplos\\n\")\n","                        if count < 2:\n","                            f.write(f\"     ¬°ADVERTENCIA! La categor√≠a '{categoria}' tiene menos de 2 ejemplos. Esto puede afectar el entrenamiento del clasificador.\\n\")\n","                else:\n","                    f.write(\"   - No hay etiquetas disponibles.\\n\")\n","            except Exception as e:\n","                f.write(f\"   - ERROR al procesar archivos de features/etiquetas: {str(e)}\\n\")\n","        else:\n","            f.write(\"   - Archivos features_tfidf.csv o etiquetas.csv no encontrados. Ejecute el extractor de caracter√≠sticas primero.\\n\")\n","\n","        f.write(\"\\n5. An√°lisis del modelo sem√°ntico (despu√©s de ejecutar gestor_modelo_semantico):\\n\")\n","        # Para un an√°lisis m√°s profundo del modelo sem√°ntico, se necesitar√≠a instanciar Ontosistema\n","        # y llamar a sus m√©todos, lo cual se hace en el flujo principal del notebook.\n","        # Aqu√≠ solo verificamos directorios del modelo sem√°ntico seg√∫n config.\n","        rutas_modelo_sem = config.get('modelo_semantico', {}).get('rutas', {})\n","        if rutas_modelo_sem:\n","            for nivel, ruta_rel in rutas_modelo_sem.items():\n","                ruta_completa_ms = os.path.join(base_dir, ruta_rel)\n","                existe_ms_dir = os.path.exists(ruta_completa_ms) and os.path.isdir(ruta_completa_ms)\n","                f.write(f\"   - Directorio para '{nivel}': {'‚úì' if existe_ms_dir else 'X'} (Ruta: {ruta_completa_ms})\\n\")\n","        else:\n","            f.write(\"   - No se encontraron rutas del modelo sem√°ntico en la configuraci√≥n.\\n\")\n","\n","        path_metricas_ontosistema = os.path.join(base_dir, 'logs_sistema', 'metricas_ontosistema.json')\n","        if os.path.exists(path_metricas_ontosistema):\n","            f.write(\"\\n6. Estad√≠sticas del modelo sem√°ntico (desde metricas_ontosistema.json):\\n\")\n","            try:\n","                with open(path_metricas_ontosistema, 'r') as mf:\n","                    metricas_data = json.load(mf)\n","                stats = metricas_data.get('estadisticas', {})\n","                for key, value in stats.items():\n","                    f.write(f\"   - {key.capitalize()}: {value}\\n\")\n","            except Exception as e:\n","                f.write(f\"   - ERROR al leer metricas_ontosistema.json: {str(e)}\\n\")\n","        else:\n","            f.write(\"\\n6. Estad√≠sticas del modelo sem√°ntico: Archivo metricas_ontosistema.json no encontrado. Ejecute la inicializaci√≥n del ontosistema.\\n\")\n","\n","        f.write(\"\\n7. Prueba de clasificaci√≥n (despu√©s de entrenar el modelo):\\n\")\n","        path_modelo_clasificador = os.path.join(base_dir, 'clasificados', 'clasificador_fenomenologico.joblib')\n","        if os.path.exists(path_modelo_clasificador):\n","            f.write(f\"   - Modelo de clasificaci√≥n guardado ('clasificador_fenomenologico.joblib'): ‚úì\\n\")\n","        else:\n","            f.write(f\"   - Modelo de clasificaci√≥n ('clasificador_fenomenologico.joblib') no encontrado. Entrene el clasificador primero.\\n\")\n","\n","        f.write(\"\\n8. Recomendaciones Generales:\\n\")\n","        f.write(\"   - Aseg√∫rese de que los archivos .txt de entrada est√©n in 'entrada_bruta'.\\n\")\n","        f.write(\"   - Ejecute todas las celdas del notebook in orden.\\n\")\n","        f.write(\"   - Verifique que 'config.yaml' est√© correctamente configurado.\\n\")\n","        f.write(\"   - Para el clasificador, asegure que cada categor√≠a tenga al menos 2 ejemplos if you use train_test_split with stratification.\\n\")\n","        f.write(\"   - Revise 'logs_sistema/' para otros logs o errores espec√≠ficos if they occur.\\n\")\n","\n","\n","        f.write(f\"\\n=== FIN DEL REPORTE ===\\n\")\n","\n","    print(f\"Reporte de an√°lisis generated in: {reporte_path}\")\n","    return reporte_path\n","\n","# Para ejecutar este script directamente (opcional, m√°s para pruebas locales)\n","# if __name__ == '__main__':\n","#     # Asumir que el script est√° en /content/YO estructural/scripts/\n","#     current_base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n","#     cfg_path = os.path.join(current_base_dir, 'configuracion', 'config.yaml')\n","#     analizar_sistema(current_base_dir, cfg_path)\n","\"\"\"\n","with open(os.path.join(BASE_DIR, 'scripts', 'analizador_sistema.py'), 'w', encoding='utf-8') as f:\n","    f.write(analizador_content)\n","print(\"Archivo 'scripts/analizador_sistema.py' creado.\")\n","\n","# --- Creaci√≥n de archivos de entrada de ejemplo con nombres para m√∫ltiples etiquetas ---\n","TEXTO_EJEMPLO_1 = \"Una sensaci√≥n de calma profunda invadi√≥ mi ser. El sol se pon√≠a, ti√±endo el cielo de naranjas y violetas. Percib√≠a el aroma de la tierra h√∫meda.\"\n","TEXTO_EJEMPLO_2 = \"Sent√≠ una punzada de angustia al recordar el evento. Las palabras resonaban en mi mente, causando un eco de tristeza. Era una emoci√≥n dif√≠cil de procesar.\"\n","TEXTO_EJEMPLO_3 = \"Reflexion√© sobre la naturaleza del tiempo y la existencia. ¬øQu√© significado tienen nuestras acciones en el gran esquema de las cosas? Una pregunta que persiste.\"\n","\n","# Renombrar para que la l√≥gica de la Celda 5 asigne diferentes etiquetas\n","with open(os.path.join(BASE_DIR, 'entrada_bruta', 'percepcion_ejemplo_1.txt'), 'w', encoding='utf-8') as f:\n","    f.write(TEXTO_EJEMPLO_1)\n","with open(os.path.join(BASE_DIR, 'entrada_bruta', 'emocion_ejemplo_1.txt'), 'w', encoding='utf-8') as f:\n","    f.write(TEXTO_EJEMPLO_2)\n","with open(os.path.join(BASE_DIR, 'entrada_bruta', 'cognicion_ejemplo_1.txt'), 'w', encoding='utf-8') as f:\n","    f.write(TEXTO_EJEMPLO_3)\n","\n","# Eliminar los archivos de ejemplo anteriores si existen\n","old_example_files = ['texto_1.txt', 'texto_2.txt', 'texto_3.txt']\n","for old_file in old_example_files:\n","    old_path = os.path.join(BASE_DIR, 'entrada_bruta', old_file)\n","    if os.path.exists(old_path):\n","        os.remove(old_path)\n","        print(f\"Archivo '{old_file}' eliminado.\")\n","\n","\n","print(\"Archivos de texto de ejemplo creados/renombrados en 'entrada_bruta' para generar m√∫ltiples etiquetas.\")\n","print(\"\\n--- Configuraci√≥n de archivos completa. Puede proceder a la siguiente celda. ---\")"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1749989992558,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"},"user_tz":360},"id":"NqcqyDRyo5Bo","outputId":"168291a8-85d8-48b6-d553-4aaa6227fc34"},"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/YO estructural/scripts' ya est√° en sys.path\n","Configuraci√≥n cargada exitosamente.\n","Error al importar m√≥dulos principales: attempted relative import with no known parent package. Aseg√∫rese de que los archivos .py est√©n en '/content/YO estructural/scripts'.\n","Contenido de sys.path: ['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/usr/local/lib/python3.11/dist-packages/setuptools/_vendor', '/root/.ipython', '/content/YO estructural/scripts']\n","Contenido de /content/YO estructural/scripts: ['analizador_sistema.py', 'modelos', 'extractor_features.py', 'gestor_modelo_semantico.py', 'clasificador.py', '__pycache__', '__init__.py']\n","Contenido de /content/YO estructural/scripts/modelos: ['contexto.py', 'macrocontexto.py', 'red_contextos.py', 'metacampo.py', 'fenomeno.py', '__init__.py', 'ontosistema.py']\n"]}],"source":["# Celda 4: Imports Principales y Configuraci√≥n de Paths\n","import sys\n","import pandas as pd\n","import numpy as np\n","import yaml\n","import os\n","\n","# --- Directorio Ra√≠z del Proyecto en Colab ---\n","BASE_DIR = '/content/YO estructural'\n","CONFIG_PATH = os.path.join(BASE_DIR, 'configuracion', 'config.yaml')\n","\n","# Agregar directorio de scripts al path de Python\n","SCRIPTS_DIR = os.path.join(BASE_DIR, 'scripts')\n","if SCRIPTS_DIR not in sys.path:\n","    sys.path.append(SCRIPTS_DIR)\n","    print(f\"'{SCRIPTS_DIR}' a√±adido a sys.path\")\n","else:\n","    print(f\"'{SCRIPTS_DIR}' ya est√° en sys.path\")\n","\n","\n","# Cargar configuraci√≥n\n","try:\n","    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n","        config = yaml.safe_load(f)\n","    print(\"Configuraci√≥n cargada exitosamente.\")\n","except FileNotFoundError:\n","    print(f\"Error: El archivo de configuraci√≥n no se encuentra en {CONFIG_PATH}. Aseg√∫rese de ejecutar la celda anterior.\")\n","    config = {} # Evitar errores posteriores, aunque el sistema no funcionar√° bien.\n","except Exception as e:\n","    print(f\"Error al cargar la configuraci√≥n: {e}\")\n","    config = {}\n","\n","# Importar clases de los scripts (despu√©s de a√±adir al path y crear archivos)\n","try:\n","    # Importaciones directas ahora que SCRIPTS_DIR est√° en sys.path\n","    from extractor_features import ExtractorFeatures\n","    from clasificador import ClasificadorFenomenologico\n","    from gestor_modelo_semantico import GestorModeloSemantico\n","    from analizador_sistema import analizar_sistema # La funci√≥n, no la clase\n","\n","    # Verificar importaciones de modelos dentro de gestor_modelo_semantico\n","    # Esto es para depurar si el error era en las importaciones relativas dentro de los scripts\n","    try:\n","        from modelos.fenomeno import Fenomeno\n","        from modelos.contexto import Contexto\n","        from modelos.macrocontexto import Macrocontexto\n","        from modelos.red_contextos import RedContextos\n","        from modelos.ontosistema import Ontosistema\n","        from modelos.metacampo import Metacampo\n","        print(\"Clases del modelo sem√°ntico importadas correctamente.\")\n","    except ImportError as e:\n","        print(f\"Error al importar clases del modelo sem√°ntico: {e}. Revise los archivos en '{os.path.join(SCRIPTS_DIR, 'modelos')}' y su __init__.py.\")\n","\n","\n","    print(\"M√≥dulos principales importados correctamente.\")\n","except ImportError as e:\n","    print(f\"Error al importar m√≥dulos principales: {e}. Aseg√∫rese de que los archivos .py est√©n en '{SCRIPTS_DIR}'.\")\n","    print(\"Contenido de sys.path:\", sys.path)\n","    print(f\"Contenido de {SCRIPTS_DIR}:\", os.listdir(SCRIPTS_DIR) if os.path.exists(SCRIPTS_DIR) else \"Directorio no encontrado\")\n","    print(f\"Contenido de {os.path.join(SCRIPTS_DIR, 'modelos')}:\", os.listdir(os.path.join(SCRIPTS_DIR, 'modelos')) if os.path.exists(os.path.join(SCRIPTS_DIR, 'modelos')) else \"Directorio no encontrado\")"]},{"cell_type":"code","execution_count":109,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1749989992575,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"},"user_tz":360},"id":"DAkxyvtNo5Bq","outputId":"b4f05c8b-54fb-410d-959b-a303501e07e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Se cargaron 3 textos.\n","  Texto 1 (etiqueta: cognicion): Reflexion√© sobre la naturaleza del tiempo y la existencia. ¬øQu√© significado tienen nuestras acciones...\n","  Texto 2 (etiqueta: emocion): Sent√≠ una punzada de angustia al recordar el evento. Las palabras resonaban en mi mente, causando un...\n","  Texto 3 (etiqueta: percepcion): Una sensaci√≥n de calma profunda invadi√≥ mi ser. El sol se pon√≠a, ti√±endo el cielo de naranjas y viol...\n","Categor√≠as definidas en config: ['percepcion', 'emocion', 'cognicion', 'relacional', 'metafisico']\n","Etiquetas cargadas de los textos: ['cognicion', 'percepcion', 'emocion']\n"]}],"source":["# Celda 5: Cargar Datos de Entrada\n","\n","# Leer textos desde la carpeta 'entrada_bruta'\n","def cargar_textos_entrada(ruta_entrada_bruta):\n","    textos = []\n","    etiquetas_textos = [] # Ejemplo: podr√≠as derivar etiquetas del nombre del archivo o de un archivo separado\n","\n","    if not os.path.exists(ruta_entrada_bruta):\n","        print(f\"Directorio de entrada '{ruta_entrada_bruta}' no encontrado.\")\n","        return textos, etiquetas_textos\n","\n","    for filename in sorted(os.listdir(ruta_entrada_bruta)):\n","        if filename.endswith('.txt'):\n","            with open(os.path.join(ruta_entrada_bruta, filename), 'r', encoding='utf-8') as f_text:\n","                textos.append(f_text.read())\n","                # L√≥gica de ejemplo para etiquetas (puedes adaptarla)\n","                if \"emocion\" in filename.lower() or \"angustia\" in filename.lower() or \"tristeza\" in filename.lower():\n","                    etiquetas_textos.append('emocion')\n","                elif \"percepcion\" in filename.lower() or \"calma\" in filename.lower() or \"sol\" in filename.lower():\n","                    etiquetas_textos.append('percepcion')\n","                elif \"reflexion\" in filename.lower() or \"cognicion\" in filename.lower() or \"tiempo\" in filename.lower():\n","                    etiquetas_textos.append('cognicion')\n","                else:\n","                    etiquetas_textos.append('otro') # Etiqueta por defecto\n","    return textos, etiquetas_textos\n","\n","RUTA_ENTRADA_BRUTA = os.path.join(BASE_DIR, 'entrada_bruta')\n","textos_crudos, etiquetas = cargar_textos_entrada(RUTA_ENTRADA_BRUTA)\n","\n","if textos_crudos:\n","    print(f\"Se cargaron {len(textos_crudos)} textos.\")\n","    for i, txt in enumerate(textos_crudos):\n","        print(f\"  Texto {i+1} (etiqueta: {etiquetas[i]}): {txt[:100]}...\")\n","else:\n","    print(\"No se cargaron textos. Aseg√∫rate de que haya archivos .txt en 'entrada_bruta'.\")\n","\n","# Asegurar que haya suficientes etiquetas para las categor√≠as principales si se usan directamente\n","if config:\n","    categorias_config = config.get('fenomenologia', {}).get('categorias_principales', [])\n","    print(f\"Categor√≠as definidas en config: {categorias_config}\")\n","    print(f\"Etiquetas cargadas de los textos: {list(set(etiquetas))}\")\n"]},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1749989992621,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"},"user_tz":360},"id":"f45YHPy-o5Br","outputId":"4ba46083-01bb-48dc-d446-549711ef3ff8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Caracter√≠sticas extra√≠das:\n","   acciones  acciones gran      al  al recordar  angustia  angustia al  \\\n","0  0.179605       0.179605  0.0000       0.0000    0.0000       0.0000   \n","1  0.000000       0.000000  0.1752       0.1752    0.1752       0.1752   \n","2  0.000000       0.000000  0.0000       0.0000    0.0000       0.0000   \n","\n","    aroma  aroma tierra   calma  calma profunda  ...    tienen  \\\n","0  0.0000        0.0000  0.0000          0.0000  ...  0.179605   \n","1  0.0000        0.0000  0.0000          0.0000  ...  0.000000   \n","2  0.1752        0.1752  0.1752          0.1752  ...  0.000000   \n","\n","   tienen nuestras  tierra  tierra h√∫meda  ti√±endo  ti√±endo cielo  tristeza  \\\n","0         0.179605  0.0000         0.0000   0.0000         0.0000    0.0000   \n","1         0.000000  0.0000         0.0000   0.0000         0.0000    0.1752   \n","2         0.000000  0.1752         0.1752   0.1752         0.1752    0.0000   \n","\n","   tristeza era  violetas  violetas percib√≠a  \n","0        0.0000    0.0000             0.0000  \n","1        0.1752    0.0000             0.0000  \n","2        0.0000    0.1752             0.1752  \n","\n","[3 rows x 96 columns]\n","Caracter√≠sticas y etiquetas guardadas en /content/YO estructural/features_extraidas\n"]}],"source":["# Celda 6: Ejecutar Extracci√≥n de Caracter√≠sticas\n","if config and 'ExtractorFeatures' in globals() and textos_crudos:\n","    extractor = ExtractorFeatures(config)\n","    features_df = extractor.extraer_features_tfidf(textos_crudos)\n","\n","    if not features_df.empty:\n","        print(\"Caracter√≠sticas extra√≠das:\")\n","        print(features_df.head())\n","        # Guardar features y etiquetas\n","        extractor.guardar_features(features_df, etiquetas, BASE_DIR)\n","    else:\n","        print(\"No se pudieron extraer caracter√≠sticas. Revisa los textos de entrada y la configuraci√≥n.\")\n","elif not textos_crudos:\n","    print(\"No hay textos cargados para extraer caracter√≠sticas. Revisa la Celda 5.\")\n","else:\n","    print(\"Extractor no disponible o configuraci√≥n no cargada. Revisa celdas anteriores.\")"]},{"cell_type":"code","execution_count":111,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1749989993038,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"},"user_tz":360},"id":"xGTwUP1Io5Bt","outputId":"c4ce7a8b-fb92-48c6-e2c7-6f14d73d6079"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape de X: (3, 96), Shape de y: (3,)\n","Advertencia: Pocos datos para el split con test_size=0.25. Se usar√° el dataset completo para entrenar y evaluar sobre √©l mismo.\n","Modelo entrenado con todos los datos.\n","Classification Report (sobre datos de entrenamiento):\n","              precision    recall  f1-score   support\n","\n","   cognicion       1.00      1.00      1.00         1\n","     emocion       1.00      1.00      1.00         1\n","  percepcion       1.00      1.00      1.00         1\n","\n","    accuracy                           1.00         3\n","   macro avg       1.00      1.00      1.00         3\n","weighted avg       1.00      1.00      1.00         3\n","\n","Modelo guardado en /content/YO estructural/clasificados\n","\n","Predicci√≥n de prueba para la primera muestra:\n","  Clase predicha: cognicion\n","  Probabilidades: [0.72 0.16 0.12]\n","  Clases del modelo: ['cognicion' 'emocion' 'percepcion']\n"]}],"source":["# Celda 7: Ejecutar Entrenamiento del Clasificador\n","if config and 'ClasificadorFenomenologico' in globals() and 'features_df' in globals() and not features_df.empty and etiquetas:\n","    clasificador = ClasificadorFenomenologico(config)\n","\n","    # Convertir DataFrame de features a NumPy array para el clasificador\n","    X = features_df.values\n","    y = np.array(etiquetas) # Asegurar que y es un array numpy\n","\n","    print(f\"Shape de X: {X.shape}, Shape de y: {y.shape}\")\n","\n","    # Verificar si hay suficientes datos y clases\n","    if X.shape[0] > 0 and len(np.unique(y)) >= 1: # Se necesita al menos 1 clase para intentar entrenar (idealmente >=2)\n","\n","        # Use the user's suggested check after calling entrenar\n","        if clasificador.entrenar(X, y): # entrenar now returns True if it was fitted\n","            clasificador.guardar_modelo(BASE_DIR)\n","\n","            # Prueba de predicci√≥n (opcional)\n","            if X.shape[0] > 0:\n","                print(\"\\nPredicci√≥n de prueba para la primera muestra:\")\n","                prediccion_ejemplo = clasificador.predecir(X[0].reshape(1, -1))\n","                probabilidades_ejemplo = clasificador.predecir_probabilidades(X[0].reshape(1, -1))\n","                print(f\"  Clase predicha: {prediccion_ejemplo[0]}\")\n","                print(f\"  Probabilidades: {probabilidades_ejemplo[0]}\")\n","                # Mostrar las clases del modelo para interpretar probabilidades\n","                if hasattr(clasificador.modelo, 'classes_'): # Still good to check in case of unforeseen issues\n","                     print(f\"  Clases del modelo: {clasificador.modelo.classes_}\")\n","        else:\n","            print(\"\\nEl modelo no fue entrenado debido a datos insuficientes (menos de 2 clases √∫nicas).\")\n","            print(\"No se realizar√° la predicci√≥n de prueba.\")\n","\n","    else:\n","        print(\"No hay suficientes datos (features o etiquetas) o clases √∫nicas para entrenar el clasificador.\")\n","        print(f\"N√∫mero de muestras: {X.shape[0]}, Clases √∫nicas en y: {len(np.unique(y)) if y.size > 0 else 0}\")\n","elif 'features_df' not in globals() or ('features_df' in globals() and features_df.empty):\n","    print(\"DataFrame de caracter√≠sticas (features_df) no disponible o vac√≠o. Ejecute la extracci√≥n de caracter√≠sticas primero.\")\n","elif not etiquetas:\n","    print(\"No hay etiquetas disponibles para entrenar el clasificador.\")\n","else:\n","    print(\"Clasificador no disponible o configuraci√≥n no cargada. Revisa celdas anteriores.\")"]},{"cell_type":"code","execution_count":112,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1749989993072,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"},"user_tz":360},"id":"wJ7ESx_Bo5Bv","outputId":"79c93361-abe2-42dd-f659-94cd1b699f86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gestor del Modelo Sem√°ntico no disponible o configuraci√≥n no cargada. Revisa celdas anteriores.\n"]}],"source":["# Celda 8: Ejecutar Gestor del Modelo Sem√°ntico\n","if config and 'GestorModeloSemantico' in globals():\n","    gestor_semantico = GestorModeloSemantico(config, BASE_DIR)\n","    gestor_semantico.inicializar_ontosistema() # Actualiza y guarda estad√≠sticas iniciales\n","    print(\"\\n--- Ejemplo de creaci√≥n de entidades sem√°nticas ---\")\n","\n","    # Crear un fen√≥meno de ejemplo\n","    fen_1 = gestor_semantico.crear_fenomeno(\n","        descripcion=\"Visi√≥n de un atardecer vibrante\",\n","        tipo=\"visual\",\n","        intensidad=0.8,\n","        polaridad=\"positivo\"\n","    )\n","\n","    # Crear un contexto de ejemplo\n","    ctx_1 = gestor_semantico.crear_contexto(\n","        nombre=\"Contemplaci√≥n vespertina\",\n","        descripcion=\"Observando el cielo al final del d√≠a\",\n","        yo_presente=True\n","    )\n","    if fen_1:\n","      ctx_1.agregar_fenomeno(fen_1.id)\n","      gestor_semantico.guardar_entidad(ctx_1, 'contextos') # Guardar de nuevo con el fen√≥meno a√±adido\n","\n","    # Crear una red de ejemplo\n","    red_1 = gestor_semantico.crear_red_contextos(\n","        nombre=\"Red de experiencias tranquilas\",\n","        descripcion=\"Contextos relacionados con la calma y la observaci√≥n\"\n","    )\n","    if ctx_1:\n","        red_1.agregar_nodo(ctx_1.id, tipo=\"contexto\", atributos={'importancia': 'alta'})\n","        # Guardar la red de nuevo si se modific√≥\n","        red_1.guardar(os.path.join(BASE_DIR, 'modelo_semantico'))\n","        gestor_semantico.ontosistema.actualizar_estadisticas() # Actualizar tras cambios en red\n","\n","    print(\"\\n--- Estad√≠sticas del Ontosistema actualizadas ---\")\n","    print(gestor_semantico.ontosistema.estadisticas)\n","    gestor_semantico.ontosistema.guardar_estadisticas()\n","\n","else:\n","    print(\"Gestor del Modelo Sem√°ntico no disponible o configuraci√≥n no cargada. Revisa celdas anteriores.\")\n"]},{"cell_type":"code","execution_count":113,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1749989993119,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"},"user_tz":360},"id":"urO17l0Yo5Bx","outputId":"2bda711a-e767-4889-b549-d58751c66f90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Funci√≥n 'analizar_sistema' no disponible o configuraci√≥n no cargada. Revisa celdas anteriores.\n"]}],"source":["# Celda 9: Ejecutar An√°lisis del Sistema y Mostrar Reporte\n","if config and 'analizar_sistema' in globals():\n","    print(\"\\n--- Ejecutando An√°lisis del Sistema ---\")\n","    ruta_reporte = analizar_sistema(BASE_DIR, CONFIG_PATH) # Pasamos BASE_DIR y CONFIG_PATH\n","\n","    if ruta_reporte and os.path.exists(ruta_reporte):\n","        print(f\"\\n--- Contenido del Reporte ({ruta_reporte}) ---\")\n","        with open(ruta_reporte, 'r', encoding='utf-8') as f_report:\n","            print(f_report.read())\n","    else:\n","        print(\"No se pudo generar o encontrar el reporte de an√°lisis.\")\n","else:\n","    print(\"Funci√≥n 'analizar_sistema' no disponible o configuraci√≥n no cargada. Revisa celdas anteriores.\")\n"]},{"cell_type":"markdown","metadata":{"id":"f85d91fc"},"source":["# Task\n","Generate a Google Colab notebook that executes the project located in the directory \"/c:/Users/Public/#...Ra√≠z Dasein/YO estructural\". The notebook should include steps for setting up the environment, installing dependencies, loading and processing data, training a classifier, managing a semantic model, and analyzing the system, following the structure and scripts provided in the project directory. The notebook should be fully executable and include explanations for each step."]},{"cell_type":"markdown","metadata":{"id":"0cc9e5a3"},"source":["## Configuraci√≥n inicial y carga de archivos\n","\n","### Subtask:\n","Agregar celdas para montar Google Drive (si los archivos est√°n all√≠), descomprimir el archivo ZIP del proyecto en el entorno de Colab, y cambiar el directorio de trabajo a la carpeta ra√≠z del proyecto (`/content/YO estructural/scripts`).\n"]},{"cell_type":"markdown","metadata":{"id":"6531aa39"},"source":["**Reasoning**:\n","The first step is to mount Google Drive to access the project files.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"678dd894","executionInfo":{"status":"ok","timestamp":1749989995949,"user_tz":360,"elapsed":2828,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"}},"outputId":"73f6ca77-c5f3-4039-d6d8-3355308b5e3b"},"source":["# Celda para montar Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","print(\"Google Drive montado en '/content/drive'\")"],"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Google Drive montado en '/content/drive'\n"]}]},{"cell_type":"markdown","metadata":{"id":"c362b429"},"source":["**Reasoning**:\n","Now that Google Drive is mounted, the next step is to navigate to the directory where the project ZIP file is located within Drive. The user needs to provide the specific path.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f9ed0274"},"source":["# Task\n","Genera un reporte t√©cnico detallado del \"Sistema Fenomenol√≥gico Estructural v2.3\" basado en la descripci√≥n de sus componentes, estructura, flujo de trabajo y funcionalidades."]},{"cell_type":"markdown","metadata":{"id":"b95d7d91"},"source":["## Introducci√≥n\n","\n","### Subtask:\n","Describir brevemente el prop√≥sito general del sistema (\"Sistema Fenomenol√≥gico Estructural v2.3\") y su objetivo de procesar textos fenomenol√≥gicos y gestionar un modelo sem√°ntico.\n"]},{"cell_type":"markdown","metadata":{"id":"fc118b06"},"source":["**Reasoning**:\n","Write a markdown cell to describe the purpose of the system as requested by the subtask.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c79f96c4"},"source":["## Estructura del proyecto y configuraci√≥n inicial\n","\n","### Subtask:\n","Explicar la organizaci√≥n de directorios (`configuracion`, `entrada_bruta`, `scripts`, `modelo_semantico`, etc.) y c√≥mo se maneja la configuraci√≥n (`config.yaml`).\n"]},{"cell_type":"markdown","metadata":{"id":"126b0641"},"source":["**Reasoning**:\n","Describe the directory structure and the role of the config.yaml file as requested by the subtask instructions.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c5c7ef82"},"source":["## Procesamiento de datos\n","\n","### Subtask:\n","Detallar los pasos para cargar los textos de entrada, el preprocesamiento (limpieza) y la extracci√≥n de caracter√≠sticas utilizando TF-IDF (`ExtractorFeatures`). Mencionar la l√≥gica de etiquetado basada en nombres de archivo.\n"]},{"cell_type":"markdown","metadata":{"id":"9f5647c4"},"source":["**Reasoning**:\n","Generate markdown text describing the data loading, preprocessing, and feature extraction steps, explaining the logic as per the instructions.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8cfcffbc"},"source":["## Clasificaci√≥n fenomenol√≥gica\n","\n","### Subtask:\n","Explicar el proceso de entrenamiento y uso del clasificador (`ClasificadorFenomenologico`), incluyendo el algoritmo utilizado (Random Forest) y c√≥mo se manejan los datos para el entrenamiento.\n"]},{"cell_type":"markdown","metadata":{"id":"bc936890"},"source":["**Reasoning**:\n","Print the Markdown section header for the classification process.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"db60e4e7"},"source":["**Reasoning**:\n","Describe the role of the ClasificadorFenomenologico class and explain the training process, including the algorithm, data handling, and data splitting.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1facaf0d"},"source":["**Reasoning**:\n","Explain how the model is saved and loaded, and describe the prediction process using the trained model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b967d756"},"source":["## Gesti√≥n del modelo sem√°ntico\n","\n","### Subtask:\n","Describir los diferentes modelos de entidades sem√°nticas (`Fenomeno`, `Contexto`, `Macrocontexto`, `RedContextos`, `Metacampo`) y el rol del `GestorModeloSemantico` en su creaci√≥n, carga y guardado. Explicar el prop√≥sito del `Ontosistema` para rastrear estad√≠sticas.\n"]},{"cell_type":"markdown","metadata":{"id":"3ccfbbf8"},"source":["**Reasoning**:\n","Print the markdown section header for the semantic model management.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49779a96","executionInfo":{"status":"ok","timestamp":1749989996030,"user_tz":360,"elapsed":24,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"}},"outputId":"07bae33c-c195-4612-f3b0-7ac25644f75a"},"source":["# Celda para describir el gestor y los modelos sem√°nticos\n","\n","print(\"## 5. Gesti√≥n del Modelo Sem√°ntico\")"],"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["## 5. Gesti√≥n del Modelo Sem√°ntico\n"]}]},{"cell_type":"markdown","metadata":{"id":"4dd6ab22"},"source":["**Reasoning**:\n","Describe the GestorModeloSemantico, the individual semantic entity models, how they are managed (created, saved, loaded), and the role of the Ontosistema, incorporating details from the provided code and Celda 8.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2837926c"},"source":["## An√°lisis del sistema\n","\n","### Subtask:\n","Describir la funci√≥n del `analizador_sistema.py` para verificar la estructura, los archivos clave, las estad√≠sticas de datos y del modelo sem√°ntico, y generar un reporte de estado.\n"]},{"cell_type":"markdown","metadata":{"id":"fa640533"},"source":["**Reasoning**:\n","Generate the markdown section describing the `analizador_sistema.py` file and its functionality, following the instructions provided.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"89630fcf"},"source":["## Flujo de trabajo en colab\n","\n","### Subtask:\n","Resumir c√≥mo se ejecutan estos componentes secuencialmente en el cuaderno de Colab.\n"]},{"cell_type":"markdown","metadata":{"id":"4d5aaab2"},"source":["**Reasoning**:\n","Print a markdown header for the workflow section and then provide a summary of the execution flow in the notebook.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8158bb13"},"source":["## Funcionalidades clave\n","\n","### Subtask:\n","Enumerar las principales capacidades del sistema basadas en los componentes implementados.\n"]},{"cell_type":"markdown","metadata":{"id":"a692ba37"},"source":["**Reasoning**:\n","Print the markdown header and then the list of key functionalities derived from the system components.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3117742c"},"source":["## Posibles extensiones/mejoras\n","\n","### Subtask:\n","Sugerir √°reas para el desarrollo futuro del sistema.\n"]},{"cell_type":"markdown","metadata":{"id":"5fcc4143"},"source":["**Reasoning**:\n","Generate markdown content suggesting future development areas for the system, addressing the points mentioned in the instructions.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0a1287c4"},"source":["## Summary:\n","\n","## Q&A\n","\n","Based on the provided solution, the system addresses the following implicit questions about the \"Sistema Fenomenol√≥gico Estructural v2.3\":\n","\n","*   What is the overall purpose and objective of the system?\n","*   How is the project structured, and how is configuration managed?\n","*   How are input texts processed, features extracted, and basic labels assigned?\n","*   How does the system classify phenomenological texts, and what algorithm is used?\n","*   What are the different semantic entity models, and how are they managed, saved, and loaded?\n","*   How does the system track statistics about the semantic model?\n","*   How do the different components of the system execute sequentially in the Colab notebook environment?\n","*   What are the main capabilities the system offers?\n","*   What are potential areas for future development or improvement?\n","\n","## Data Analysis Key Findings\n","\n","*   The system is designed to process phenomenological texts, extract meaningful information, and build/manage a structured semantic model.\n","*   The project follows a specific directory structure (`configuracion`, `entrada_bruta`, `scripts`, `modelo_semantico`, etc.) to organize components and data.\n","*   System parameters are centrally managed via a `config.yaml` file, covering processing, classification, phenomenology, system, and semantic model settings.\n","*   Text data loading (`.txt` files from `entrada_bruta`) includes a basic labeling logic based on keywords in filenames (e.g., \"emocion\" -> 'emocion').\n","*   Text preprocessing includes lowercasing and removing digits/extra spaces before feature extraction.\n","*   Features are extracted using TF-IDF vectorization (`ExtractorFeatures`), controlled by `tfidf_max_features` and `tfidf_ngram_range` from the configuration.\n","*   A `ClasificadorFenomenologico` component, using `RandomForestClassifier` by default, is responsible for classifying texts into predefined categories.\n","*   The classifier training process attempts to use `train_test_split` with stratification based on the `test_size` config parameter.\n","*   The system includes several semantic entity models: `Fenomeno`, `Contexto`, `Macrocontexto`, `RedContextos` (using `networkx` for graphs), and `Metacampo`, each with defined attributes.\n","*   The `GestorModeloSemantico` handles the creation, saving (as YAML files in specified directories), and loading of these semantic entities.\n","*   An `Ontosistema` component tracks statistics about the semantic model, such as the count of each entity type and 'YO' occurrences, persisting them to `metricas_ontosistema.json`.\n","*   An `analizador_sistema` script verifies the system's state, checking directory structure, key files, processed data stats (e.g., total texts, features), and semantic model stats, generating a consolidated `reporte_analisis.txt`.\n","*   The Colab notebook workflow executes these components sequentially, from setup and data loading (Celdas 2-5) through processing, classification, and semantic modeling (Celdas 6-8), ending with system analysis (Celda 9).\n","\n","## Insights or Next Steps\n","\n","*   Enhance the labeling process beyond filename keywords to support more accurate and flexible classification training, potentially exploring manual annotation tools or active learning strategies.\n","*   Deepen the semantic model analysis and visualization capabilities, especially for the `RedContextos`, to provide more insightful understanding of the relationships and emergent patterns within the phenomenological data.\n"]},{"cell_type":"markdown","metadata":{"id":"f5204295"},"source":["## Reporte T√©cnico: Sistema Fenomenol√≥gico Estructural v2.3\n","\n","El **Sistema Fenomenol√≥gico Estructural v2.3** es una herramienta dise√±ada para el an√°lisis y procesamiento de textos fenomenol√≥gicos. Su prop√≥sito principal es extraer informaci√≥n significativa de descripciones de experiencias subjetivas y construir y gestionar un modelo sem√°ntico estructurado basado en estos datos. El sistema busca identificar patrones, relaciones y conceptos emergentes dentro del material textual, proporcionando una representaci√≥n formalizada y analizable de los fen√≥menos descritos."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6034f5e6","executionInfo":{"status":"ok","timestamp":1749990000409,"user_tz":360,"elapsed":4373,"user":{"displayName":"elliot allderson","userId":"17788712019039927645"}},"outputId":"de1b4c85-f8ef-4bda-af12-4d6a8f4e8804"},"source":["import os\n","\n","# Definir la ruta completa del archivo en Google Drive\n","report_path_in_drive = '/content/drive/My Drive/reporte_tecnico_sistema_fenomenologico.md'\n","\n","# Recopilar el contenido del reporte t√©cnico de las salidas de las celdas Markdown\n","# (Esto asume que las celdas Markdown anteriores han sido ejecutadas y su contenido est√° en el historial)\n","# En un escenario real con ejecuci√≥n secuencial, recopilar√≠as el texto generado en cada paso.\n","\n","# Aqu√≠, simular√© la recopilaci√≥n del contenido basado en las secciones generadas previamente:\n","report_content = \"\"\"\n","## Reporte T√©cnico: Sistema Fenomenol√≥gico Estructural v2.3\n","\n","El **Sistema Fenomenol√≥gico Estructural v2.3** es una herramienta dise√±ada para el an√°lisis y procesamiento de textos fenomenol√≥gicos. Su prop√≥sito principal es extraer informaci√≥n significativa de descripciones de experiencias subjetivas y construir y gestionar un modelo sem√°ntico estructurado basado en estos datos. El sistema busca identificar patrones, relaciones y conceptos emergentes dentro del material textual, proporcionando una representaci√≥n formalizada y analizable de los fen√≥menos descritos.\n","\n","## 2. Estructura de Directorios y Configuraci√≥n\n","\n","### 2.1 Organizaci√≥n de Directorios\n","\n","El proyecto \"YO estructural\" sigue una estructura de directorios organizada para separar los diferentes componentes y tipos de datos:\n","\n","*   **/content/YO estructural/**: Es el directorio ra√≠z del proyecto en el entorno de Colab.\n","*   **/content/YO estructural/configuracion/**: Contiene el archivo `config.yaml` con todos los par√°metros de configuraci√≥n del sistema.\n","*   **/content/YO estructural/entrada_bruta/**: Aqu√≠ se deben colocar los archivos de texto crudo (`.txt`) que ser√°n procesados por el sistema.\n","*   **/content/YO estructural/scripts/**: Almacena los scripts de Python que implementan la l√≥gica del sistema (extracci√≥n de caracter√≠sticas, clasificaci√≥n, gesti√≥n del modelo sem√°ntico, an√°lisis). Este directorio se a√±ade al `sys.path` para permitir la importaci√≥n de los m√≥dulos.\n","*   **/content/YO estructural/scripts/modelos/**: Subdirectorio dentro de `scripts` que contiene las definiciones de las clases del modelo sem√°ntico (Fenomeno, Contexto, Macrocontexto, etc.).\n","*   **/content/YO estructural/features_extraidas/**: Directorio de salida donde se guardan los datos despu√©s de la extracci√≥n de caracter√≠sticas (e.g., `features_tfidf.csv`, `etiquetas.csv`).\n","*   **/content/YO estructural/clasificados/**: Directorio de salida donde se guarda el modelo de clasificaci√≥n entrenado (e.g., `clasificador_fenomenologico.joblib`).\n","*   **/content/YO estructural/logs_sistema/**: Contiene archivos de registro y reportes generados por el sistema (e.g., `reporte_analisis.txt`, `metricas_ontosistema.json`).\n","*   **/content/YO estructural/modelo_semantico/**: Contiene subdirectorios para almacenar las diferentes entidades del modelo sem√°ntico persistidas en archivos (e.g., `fenomenos/`, `contextos/`, `redes/`).\n","\n","### 2.2 Archivo de Configuraci√≥n (`config.yaml`)\n","\n","El archivo `config.yaml` es fundamental para el funcionamiento del sistema, ya que centraliza todos los par√°metros configurables. Est√° organizado en secciones principales:\n","\n","*   **`procesamiento`**: Define par√°metros para la limpieza y extracci√≥n de caracter√≠sticas de los textos, como el idioma, la longitud m√≠nima del texto, y la configuraci√≥n del vectorizador TF-IDF (`tfidf_max_features`, `tfidf_ngram_range`).\n","*   **`clasificacion`**: Contiene la configuraci√≥n para el algoritmo de clasificaci√≥n, incluyendo el tipo de algoritmo (`algoritmo`), el n√∫mero de estimadores (`n_estimators`), el tama√±o del conjunto de prueba (`test_size`) y la semilla aleatoria (`random_state`).\n","*   **`fenomenologia`**: Define aspectos relacionados con la estructura fenomenol√≥gica, como las categor√≠as principales (`categorias_principales`), umbrales de relevancia de t√©rminos y opciones para generar nodos de Obsidian.\n","*   **`sistema`**: Par√°metros generales del sistema, como la configuraci√≥n de backups autom√°ticos y la generaci√≥n de reportes de an√°lisis.\n","*   **`modelo_semantico`**: Configuraci√≥n detallada para el modelo sem√°ntico, incluyendo las rutas relativas para guardar las entidades (`rutas`), umbrales para el agrupamiento y emergencia de conceptos, y la configuraci√≥n para la integraci√≥n con Neo4j (si est√° activada).\n","\n","Esta configuraci√≥n permite ajustar el comportamiento del sistema sin modificar directamente el c√≥digo de los scripts.\n","\n","### 2.3 Carga y Uso de la Configuraci√≥n\n","\n","Al inicio de la ejecuci√≥n principal del notebook (Celda 4), el sistema carga el contenido del archivo `config.yaml` en una variable de diccionario llamada `config`. Esta variable `config` se pasa luego a las clases y funciones relevantes (como `ExtractorFeatures`, `ClasificadorFenomenologico`, `GestorModeloSemantico` y `analizador_sistema`) para que utilicen los par√°metros definidos en la configuraci√≥n. Esto asegura que el comportamiento del sistema est√© alineado con las especificaciones del usuario.\n","\n","## 3. Carga de Datos, Preprocesamiento y Extracci√≥n de Caracter√≠sticas\n","\n","### 3.1 Carga de Textos de Entrada\n","\n","El sistema comienza procesando los textos fenomenol√≥gicos crudos proporcionados por el usuario. Estos textos deben estar en archivos `.txt` ubicados dentro del directorio `/content/YO estructural/entrada_bruta/`.\n","\n","La funci√≥n `cargar_textos_entrada` (definida y utilizada en la Celda 5) es responsable de leer cada uno de estos archivos. Itera sobre los archivos en el directorio de entrada, lee su contenido y lo almacena en una lista (`textos_crudos`).\n","\n","### 3.2 L√≥gica de Etiquetado B√°sico\n","\n","Para permitir el entrenamiento inicial de un clasificador, la funci√≥n `cargar_textos_entrada` implementa una l√≥gica simple de etiquetado. Esta l√≥gica asigna una etiqueta a cada texto bas√°ndose en la presencia de ciertas palabras clave en el **nombre del archivo** (no en el contenido del texto).\n","\n","Por ejemplo:\n","*   Si el nombre del archivo contiene \"emocion\", \"angustia\" o \"tristeza\" (insensible a may√∫sculas/min√∫sculas), se le asigna la etiqueta `'emocion'`.\n","*   Si contiene \"percepcion\", \"calma\" o \"sol\", se le asigna la etiqueta `'percepcion'`.\n","*   Si contiene \"reflexion\", \"cognicion\" o \"tiempo\", se le asigna la etiqueta `'cognicion'`.\n","*   Para cualquier otro caso, se asigna una etiqueta por defecto como `'otro'`.\n","\n","Esta es una forma b√°sica de obtener etiquetas a partir de datos no etiquetados expl√≠citamente y es √∫til para demostraciones o prototipos r√°pidos. En un escenario real, las etiquetas podr√≠an provenir de una fuente externa o de un proceso de anotaci√≥n manual. Las etiquetas recolectadas se almacenan en la lista `etiquetas`.\n","\n","### 3.3 Extracci√≥n de Caracter√≠sticas con `ExtractorFeatures`\n","\n","Una vez cargados los textos, el siguiente paso es transformar estos datos textuales en un formato num√©rico que pueda ser utilizado por algoritmos de aprendizaje autom√°tico. Esta tarea la realiza la clase `ExtractorFeatures` (definida en `scripts/extractor_features.py` y utilizada en la Celda 6).\n","\n","`ExtractorFeatures` se inicializa con el diccionario de configuraci√≥n (`config`), lo que le permite acceder a par√°metros relevantes definidos por el usuario en `config.yaml`, como:\n","*   `tfidf_max_features`: El n√∫mero m√°ximo de caracter√≠sticas (t√©rminos o n-gramas) a considerar.\n","*   `tfidf_ngram_range`: El rango de n-gramas a extraer (ej. `[1, 2]` para unigramas y bigramas).\n","*   `idioma`: Utilizado para cargar una lista b√°sica de \"stop words\" (palabras comunes que se ignoran).\n","\n","### 3.4 Proceso de Preprocesamiento de Texto\n","\n","Antes de la vectorizaci√≥n, `ExtractorFeatures` aplica un proceso de limpieza a cada texto utilizando el m√©todo `limpiar_texto`. Este proceso incluye:\n","*   Convertir el texto a min√∫sculas.\n","*   Eliminar d√≠gitos num√©ricos.\n","*   Eliminar espacios en blanco extra y saltos de l√≠nea, dejando solo un espacio entre palabras.\n","*   (Se podr√≠an a√±adir pasos adicionales como eliminaci√≥n de puntuaci√≥n o lematizaci√≥n si fuera necesario).\n","\n","Este preprocesamiento asegura que la vectorizaci√≥n se base en una representaci√≥n consistente y limpia de los textos.\n","\n","### 3.5 Vectorizaci√≥n TF-IDF\n","\n","La transformaci√≥n principal se realiza utilizando `TfidfVectorizer` de la biblioteca `scikit-learn`. El m√©todo `extraer_features_tfidf` de `ExtractorFeatures` toma la lista de textos limpios y aplica el proceso TF-IDF (Term Frequency-Inverse Document Frequency).\n","\n","TF-IDF es una t√©cnica que refleja la importancia de una palabra en un documento en relaci√≥n con una colecci√≥n de documentos (corpus). Las palabras que son frecuentes en un documento pero raras en el resto del corpus obtienen puntuaciones TF-IDF m√°s altas, indicando que son m√°s relevantes para ese documento particular.\n","\n","`TfidfVectorizer` construye un vocabulario de t√©rminos a partir del corpus (limitado por `tfidf_max_features` y `tfidf_ngram_range`) y crea una matriz donde cada fila representa un documento y cada columna representa un t√©rmino del vocabulario. Los valores en la matriz son las puntuaciones TF-IDF. El resultado se convierte en un `pandas.DataFrame` (`features_df`), donde las columnas son los t√©rminos extra√≠dos.\n","\n","### 3.6 Almacenamiento de Caracter√≠sticas y Etiquetas\n","\n","Finalmente, el m√©todo `guardar_features` de `ExtractorFeatures` se encarga de guardar los resultados de la extracci√≥n y las etiquetas asociadas. El DataFrame de caracter√≠sticas (`features_df`) se guarda como un archivo CSV (`features_tfidf.csv`) y la lista de etiquetas (`etiquetas`) se guarda en otro archivo CSV simple (`etiquetas.csv`). Ambos archivos se almacenan en el directorio `/content/YO estructural/features_extraidas/`. Esto permite persistir los datos procesados para su uso posterior, por ejemplo, para entrenar o evaluar modelos sin tener que repetir la carga y extracci√≥n.\n","\n","## 4. Clasificaci√≥n Fenomenol√≥gica\n","\n","El componente clave para asignar categor√≠as fenomenol√≥gicas a los textos es la clase `ClasificadorFenomenologico` (definida en `scripts/clasificador.py` y utilizada en la Celda 7).\n","\n","### 4.1 Rol del Clasificador Fenomenol√≥gico\n","\n","La clase `ClasificadorFenomenologico` encapsula la l√≥gica para entrenar y utilizar un modelo de aprendizaje autom√°tico capaz de predecir la categor√≠a fenomenol√≥gica (como 'percepcion', 'emocion', 'cognicion', etc.) de un texto bas√°ndose en sus caracter√≠sticas extra√≠das. Se inicializa con el diccionario `config`, lo que le permite acceder a los par√°metros espec√≠ficos para el entrenamiento del modelo definidos en la secci√≥n `clasificacion` de `config.yaml`.\n","\n","### 4.2 Proceso de Entrenamiento (`entrenar` m√©todo)\n","\n","El entrenamiento del modelo se lleva a cabo mediante el m√©todo `entrenar`. Este m√©todo recibe dos argumentos principales:\n","*   `X`: Una matriz o DataFrame que contiene las caracter√≠sticas num√©ricas de los textos de entrenamiento. En el flujo del notebook, este es el `features_df` generado por el `ExtractorFeatures` (convertido a un array NumPy).\n","*   `y`: Un array o lista que contiene las etiquetas de clase correspondientes a cada muestra en `X`. En el notebook, este es el array `etiquetas` cargado o derivado de los nombres de archivo.\n","\n","### 4.3 Algoritmo: Random Forest\n","\n","El algoritmo de clasificaci√≥n utilizado por defecto en `ClasificadorFenomenologico` es **Random Forest (`sklearn.ensemble.RandomForestForestClassifier`)**. Este es un algoritmo de ensemble que construye m√∫ltiples √°rboles de decisi√≥n durante el entrenamiento y produce la clase que es la moda de las clases (clasificaci√≥n) o la predicci√≥n media (regresi√≥n) de los √°rboles individuales.\n","\n","El comportamiento del Random Forest se configura mediante par√°metros obtenidos de `config.yaml`, como:\n","*   `n_estimators`: El n√∫mero de √°rboles en el bosque (por defecto 100).\n","*   `random_state`: Un valor para inicializar la semilla aleatoria, asegurando la reproducibilidad de los resultados.\n","\n","### 4.4 Divisi√≥n de Datos para Entrenamiento y Prueba\n","\n","Idealmente, para evaluar el rendimiento del modelo de manera imparcial, los datos se dividen en conjuntos de entrenamiento y prueba. El m√©todo `entrenar` utiliza `sklearn.model_selection.train_test_split` para realizar esta divisi√≥n.\n","\n","El tama√±o del conjunto de prueba se controla mediante el par√°metro `test_size` del `config.yaml` (por defecto 0.25, es decir, 25% de los datos para prueba).\n","\n","El c√≥digo intenta utilizar **estratificaci√≥n** (`stratify=y`) si hay suficientes clases √∫nicas y suficientes ejemplos por clase. La estratificaci√≥n asegura que la proporci√≥n de cada clase sea aproximadamente la misma en los conjuntos de entrenamiento y prueba que en el conjunto de datos original. Esto es particularmente importante cuando se trabaja con conjuntos de datos desequilibrados (donde algunas clases tienen muchos m√°s ejemplos que otras).\n","\n","### 4.5 Manejo de Datos Insuficientes\n","\n","El m√©todo `entrenar` incluye verificaciones para manejar situaciones con datos limitados. Si el n√∫mero total de muestras es insuficiente o si hay menos de 2 clases √∫nicas, la divisi√≥n estratificada no es posible o no tiene sentido, y el modelo puede no ser entrenado correctamente o se entrenar√° y evaluar√° sobre el mismo conjunto de datos (como se ve en la Celda 7 con los datos de ejemplo limitados). El m√©todo imprime advertencias o errores en estos casos para informar al usuario.\n","\n","### 4.6 Guardado y Carga del Modelo\n","\n","Una vez entrenado el modelo, es posible guardarlo para su uso futuro sin necesidad de reentrenar. El m√©todo `guardar_modelo` utiliza la biblioteca `joblib` para serializar el objeto del modelo y guardarlo en un archivo (`clasificador_fenomenologico.joblib`) dentro del directorio `/content/YO estructural/clasificados/`.\n","\n","De manera inversa, el m√©todo `cargar_modelo` permite cargar un modelo previamente guardado desde este mismo archivo. Esto es √∫til para desplegar el modelo entrenado en un entorno de producci√≥n o para continuar trabajando con √©l en una sesi√≥n diferente.\n","\n","### 4.7 Proceso de Predicci√≥n (`predecir` y `predecir_probabilidades`)\n","\n","El prop√≥sito final del clasificador es predecir la categor√≠a fenomenol√≥gica de nuevos textos. Esto se realiza mediante los m√©todos `predecir` y `predecir_probabilidades`, ambos toman como entrada las caracter√≠sticas (`X_nuevos`) de las nuevas muestras (obtenidas tambi√©n mediante `ExtractorFeatures`).\n","\n","*   `predecir(X_nuevos)`: Este m√©todo devuelve la clase predicha (la etiqueta de categor√≠a) para cada muestra en `X_nuevos`. La salida es un array de etiquetas.\n","*   `predecir_probabilidades(X_nuevos)`: Este m√©todo devuelve las probabilidades de que cada muestra pertenezca a cada una de las clases conocidas por el modelo. La salida es una matriz donde cada fila corresponde a una muestra y cada columna corresponde a una clase, conteniendo la probabilidad predicha para esa clase. Las columnas est√°n ordenadas seg√∫n el atributo `classes_` del modelo entrenado.\n","\n","La Celda 7 muestra un ejemplo de c√≥mo se utilizan estos m√©todos para predecir la clase y las probabilidades para la primera muestra del conjunto de entrenamiento despu√©s de que el modelo ha sido entrenado.\n","\n","## 5. Gesti√≥n del Modelo Sem√°ntico\n","\n","El coraz√≥n de la representaci√≥n del conocimiento en el sistema es el **Modelo Sem√°ntico**, gestionado principalmente por la clase `GestorModeloSemantico` (definida en `scripts/gestor_modelo_semantico.py` y utilizada en la Celda 8).\n","\n","### 5.1 Rol del GestorModeloSemantico\n","\n","El `GestorModeloSemantico` act√∫a como una interfaz centralizada para interactuar con las diferentes entidades que componen el modelo sem√°ntico. Se inicializa con la configuraci√≥n general (`config`) y el directorio base del proyecto (`base_dir`). Esto le permite conocer las rutas donde deben almacenarse las diferentes entidades sem√°nticas, seg√∫n lo especificado en la secci√≥n `modelo_semantico.rutas` del `config.yaml`.\n","\n","Una de sus funciones clave es la inicializaci√≥n y gesti√≥n del **Ontosistema**, una clase que rastrea estad√≠sticas generales del modelo sem√°ntico. El gestor asegura que los directorios necesarios para almacenar las entidades existan al inicializarse (`_crear_directorios_modelo`).\n","\n","### 5.2 Modelos de Entidades Sem√°nticas\n","\n","El modelo sem√°ntico est√° compuesto por varias clases de entidades, cada una representando un tipo de informaci√≥n estructurada extra√≠da o construida a partir de los textos fenomenol√≥gicos:\n","\n","*   **`Fenomeno`** (`scripts/modelos/fenomeno.py`): Representa una experiencia subjetiva discreta o un evento perceptual/cognitivo/emocional. Sus atributos clave incluyen:\n","    *   `id`: Identificador √∫nico.\n","    *   `descripcion`: Texto que describe el fen√≥meno.\n","    *   `tipo`: Categor√≠a del fen√≥meno (ej. 'visual', 'auditivo', 'pensamiento').\n","    *   `intensidad`: Valor num√©rico (0-1) indicando la fuerza o prominencia del fen√≥meno.\n","    *   `polaridad`: Sentimiento asociado (ej. 'positivo', 'negativo', 'neutra').\n","    *   `conceptos_asociados`: Lista de strings (palabras clave).\n","    *   `timestamp`: Momento de creaci√≥n.\n","\n","*   **`Contexto`** (`scripts/modelos/contexto.py`): Agrupa uno o m√°s `Fenomeno`s que ocurren juntos o est√°n relacionados tem√°ticamente. Representa una \"situaci√≥n\" o \"escena\" experiencial. Atributos clave:\n","    *   `id`: Identificador √∫nico.\n","    *   `nombre`: Nombre descriptivo del contexto.\n","    *   `descripcion`: Descripci√≥n detallada.\n","    *   `fenomenos_ids`: Lista de IDs de los `Fenomeno`s contenidos en este contexto.\n","    *   `macrocontexto_id`: ID del `Macrocontexto` al que pertenece (si aplica).\n","    *   `relaciones`: Diccionario para vincular este contexto con otros ({id_otro_contexto: tipo_relacion}).\n","    *   `timestamp`: Momento de creaci√≥n.\n","    *   `yo_presente`: Booleano que indica si el \"YO\" (la primera persona) est√° expl√≠citamente presente o implicado en este contexto.\n","\n","*   **`Macrocontexto`** (`scripts/modelos/macrocontexto.py`): Un nivel superior de abstracci√≥n que agrupa varios `Contexto`s relacionados. Representa temas o dominios experienciales m√°s amplios. Atributos clave:\n","    *   `id`: Identificador √∫nico.\n","    *   `nombre`: Nombre del macrocontexto.\n","    *   `descripcion`: Descripci√≥n.\n","    *   `contextos_ids`: Lista de IDs de los `Contexto`s que contiene.\n","    *   `temas_centrales`: Lista de strings.\n","    *   `timestamp`: Momento de creaci√≥n.\n","\n","*   **`RedContextos`** (`scripts/modelos/red_contextos.py`): Representa la estructura relacional entre `Contexto`s (y potencialmente otras entidades) como un grafo. Utiliza la biblioteca `networkx`. Atributos clave:\n","    *   `id`: Identificador √∫nico.\n","    *   `nombre`: Nombre de la red.\n","    *   `descripcion`: Descripci√≥n.\n","    *   `grafo`: Objeto `networkx.DiGraph` que almacena nodos (entidades) y aristas (relaciones) con sus atributos.\n","    *   `timestamp`: Momento de creaci√≥n.\n","    *   `propiedades_emergentes`: Diccionario para almacenar propiedades calculadas a partir de la estructura de la red (ej. centralidad).\n","\n","*   **`Metacampo`** (`scripts/modelos/metacampo.py`): Representa un concepto o dominio de significado emergente que trasciende las entidades individuales, posiblemente derivado de patrones en la red o agrupaciones de conceptos. Atributos clave:\n","    *   `id`: Identificador √∫nico.\n","    *   `nombre`: Nombre del metacampo.\n","    *   `descripcion`: Descripci√≥n.\n","    *   `conceptos_clave`: Lista de conceptos principales asociados.\n","    *   `entidades_relacionadas_ids`: IDs de otras entidades (fenomenos, contextos, etc.) vinculadas a este metacampo.\n","    *   `timestamp`: Momento de creaci√≥n.\n","\n","### 5.3 Creaci√≥n, Guardado y Carga de Entidades\n","\n","El `GestorModeloSemantico` proporciona m√©todos (`crear_fenomeno`, `crear_contexto`, `crear_macrocontexto`, `crear_red_contextos`) para instanciar estas clases de entidades.\n","\n","El m√©todo `guardar_entidad` es central para la persistencia. Toma un objeto de entidad (que debe tener un m√©todo `to_dict`) y el tipo de entidad (como string, ej. 'fenomenos', 'contextos'). Utiliza la ruta absoluta correspondiente obtenida de la configuraci√≥n y guarda la representaci√≥n en diccionario de la entidad como un archivo YAML (`.yaml`) con un nombre basado en el tipo y el ID de la entidad (ej. `fenomenos/fenomenos_abcdef12.yaml`). El formato YAML es elegido por su legibilidad.\n","\n","El m√©todo `cargar_entidad` realiza la operaci√≥n inversa. Dado un ID de entidad y su tipo, construye la ruta del archivo esperado, lee el archivo YAML y utiliza el m√©todo de clase `from_dict` de la entidad correspondiente para recrear el objeto en memoria.\n","\n","La Celda 8 demuestra la creaci√≥n de instancias de `Fenomeno`, `Contexto` y `RedContextos`, y c√≥mo el gestor se utiliza para guardarlas en sus respectivos directorios YAML.\n","\n","### 5.4 El Ontosistema para Estad√≠sticas\n","\n","La clase `Ontosistema` (definida en `scripts/modelos/ontosistema.py` y gestionada por `GestorModeloSemantico`) tiene como prop√≥sito principal rastrear y reportar estad√≠sticas sobre el estado actual del modelo sem√°ntico.\n","\n","Se inicializa con la configuraci√≥n y el directorio base. Su m√©todo clave `actualizar_estadisticas` recorre los directorios de almacenamiento de las entidades sem√°nticas (usando las rutas de `config.yaml`) y cuenta el n√∫mero de archivos YAML presentes para cada tipo de entidad (fen√≥menos, contextos, macrocontextos, redes, metacampos). Tambi√©n incluye una l√≥gica espec√≠fica para contar cu√°ntos `Contexto`s tienen el indicador `yo_presente` activado.\n","\n","Estas estad√≠sticas se almacenan en un diccionario interno (`self.estadisticas`) junto con un `timestamp` de la √∫ltima actualizaci√≥n.\n","\n","El m√©todo `guardar_estadisticas` serializa este diccionario de estad√≠sticas y el timestamp a un archivo JSON (`metricas_ontosistema.json`) ubicado en el directorio `logs_sistema`.\n","\n","La Celda 8 llama a `gestor_semantico.inicializar_ontosistema()`, que actualiza y guarda las estad√≠sticas al inicio, y luego vuelve a actualizar y guardar las estad√≠sticas despu√©s de crear las entidades de ejemplo, demostrando c√≥mo el Ontosistema refleja el crecimiento del modelo.\n","\n","## 6. An√°lisis del Sistema\n","\n","El sistema incluye una herramienta para verificar su estado y generar un reporte de an√°lisis. Esta funci√≥n est√° centralizada en el script `analizador_sistema.py`.\n","\n","### 6.1 Rol del Analizador del Sistema\n","\n","La funci√≥n principal del script `analizador_sistema.py` es proporcionar una visi√≥n general del estado operativo del \"Sistema Fenomenol√≥gico Estructural\". La funci√≥n clave dentro de este script es `analizar_sistema`, la cual se encarga de realizar varias verificaciones y consolidar la informaci√≥n en un reporte de texto.\n","\n","### 6.2 Verificaciones Realizadas\n","\n","La funci√≥n `analizar_sistema` lleva a cabo una serie de verificaciones esenciales para evaluar la salud del sistema:\n","\n","*   **Verificaci√≥n de Estructura de Directorios**: Comprueba la existencia de los directorios base esperados (como `entrada_bruta`, `features_extraidas`, `clasificados`, `logs_sistema`, `configuracion`, `modelo_semantico`) para asegurar que la estructura del proyecto est√° intacta.\n","*   **Verificaci√≥n de Archivos Clave**: Confirma la presencia de archivos cr√≠ticos como `config.yaml`. Tambi√©n verifica si los archivos generados en etapas anteriores, como `features_tfidf.csv`, `etiquetas.csv` y el modelo de clasificaci√≥n guardado (`clasificador_fenomenologico.joblib`), existen en sus ubicaciones esperadas.\n","*   **An√°lisis de Datos Procesados**: Si los archivos `features_tfidf.csv` y `etiquetas.csv` son encontrados, el analizador los carga para reportar estad√≠sticas sobre los datos que han sido procesados. Esto incluye el n√∫mero total de textos procesados, el n√∫mero de caracter√≠sticas extra√≠das (columnas en el DataFrame de features), el n√∫mero total de etiquetas y el n√∫mero de categor√≠as √∫nicas presentes. Tambi√©n informa sobre la distribuci√≥n de cada categor√≠a, destacando advertencias si alguna categor√≠a tiene menos de 2 ejemplos, lo cual es crucial para el entrenamiento estratificado del clasificador.\n","*   **An√°lisis del Modelo Sem√°ntico**: El analizador verifica si los directorios para cada nivel del modelo sem√°ntico (fen√≥menos, contextos, macrocontextos, etc.), definidos en `config.yaml`, existen. Si el archivo de estad√≠sticas del Ontosistema (`metricas_ontosistema.json`) ha sido generado (por el `GestorModeloSemantico`), el analizador lo carga para incluir en el reporte las estad√≠sticas recopiladas por el Ontosistema, como el n√∫mero de entidades de cada tipo y las apariciones del \"YO\".\n","\n","### 6.3 Generaci√≥n del Reporte\n","\n","Todos los hallazgos de estas verificaciones se escriben en un archivo de texto plano llamado `reporte_analisis.txt`, el cual se guarda en el directorio `logs_sistema/`. Este reporte es un resumen estructurado del estado del sistema, √∫til para depuraci√≥n y monitoreo.\n","\n","### 6.4 Ejecuci√≥n en el Notebook\n","\n","La funci√≥n `analizar_sistema` se ejecuta en la Celda 9 del notebook, despu√©s de que las etapas de extracci√≥n de caracter√≠sticas, clasificaci√≥n y gesti√≥n del modelo sem√°ntico han tenido la oportunidad de generar sus respectivos archivos. La Celda 9 tambi√©n se encarga de leer y mostrar en la salida del notebook el contenido completo del `reporte_analisis.txt` generado, permitiendo al usuario revisar el estado del sistema inmediatamente.\n","\n","## 7. Flujo de Trabajo en Colab\n","\n","El cuaderno de Google Colab est√° estructurado para ejecutar el Sistema Fenomenol√≥gico Estructural de manera secuencial, asegurando que cada paso se base en los resultados del anterior. El flujo t√≠pico de ejecuci√≥n sigue las siguientes celdas principales:\n","\n","*   **Celda 2: Instalaci√≥n de Dependencias**: Este es el primer paso funcional, asegurando que todas las bibliotecas de Python necesarias (`pandas`, `numpy`, `scikit-learn`, `pyyaml`, `networkx`) est√©n instaladas en el entorno de Colab.\n","*   **Celda 3: Creaci√≥n de Estructura de Directorios y Archivos Esenciales**: Prepara el entorno creando la estructura de carpetas esperada (`entrada_bruta`, `scripts`, `modelo_semantico`, etc.) y generando archivos clave como `config.yaml` y los archivos `.py` de los scripts principales (`extractor_features.py`, `clasificador.py`, `gestor_modelo_semantico.py`, `analizador_sistema.py`, y los modelos sem√°nticos en `scripts/modelos/`). Tambi√©n crea archivos de texto de ejemplo en `entrada_bruta` con nombres que permiten una etiquetado b√°sico.\n","*   **Celda 4: Imports Principales y Configuraci√≥n de Paths**: Configura el entorno de Python a√±adiendo el directorio `scripts` al `sys.path` para que los m√≥dulos puedan ser importados. Luego, carga el archivo `config.yaml` en la variable `config`, que ser√° utilizada por los componentes subsiguientes. Finalmente, intenta importar las clases y funciones principales de los scripts.\n","*   **Celda 5: Cargar Datos de Entrada**: Lee los archivos `.txt` del directorio `entrada_bruta` y extrae su contenido. Tambi√©n aplica una l√≥gica simple basada en los nombres de archivo para generar etiquetas de categor√≠a asociadas a cada texto. Los resultados son las listas `textos_crudos` y `etiquetas`.\n","*   **Celda 6: Ejecutar Extracci√≥n de Caracter√≠sticas**: Utiliza la clase `ExtractorFeatures`, inicializada con la configuraci√≥n, para limpiar los `textos_crudos` y aplicar la vectorizaci√≥n TF-IDF. El resultado es un DataFrame (`features_df`) que representa los textos como vectores num√©ricos. Este DataFrame y las `etiquetas` se guardan en archivos CSV en el directorio `features_extraidas`.\n","*   **Celda 7: Ejecutar Entrenamiento del Clasificador**: Instancia la clase `ClasificadorFenomenologico` con la configuraci√≥n. Carga el `features_df` y las `etiquetas` (si no est√°n ya en memoria) y entrena el modelo de clasificaci√≥n (Random Forest) utilizando estos datos. Maneja casos con datos insuficientes e imprime un reporte de clasificaci√≥n. El modelo entrenado se guarda en un archivo (`.joblib`) en el directorio `clasificados`. Opcionalmente, realiza una predicci√≥n de prueba.\n","*   **Celda 8: Ejecutar Gestor del Modelo Sem√°ntico**: Inicializa la clase `GestorModeloSemantico`, que a su vez inicializa el `Ontosistema`. Esta celda demuestra la creaci√≥n de entidades sem√°nticas de ejemplo (`Fenomeno`, `Contexto`, `RedContextos`) utilizando los m√©todos del gestor. Las entidades creadas se guardan como archivos YAML en los directorios del modelo sem√°ntico. El `Ontosistema` actualiza y guarda estad√≠sticas sobre el modelo sem√°ntico a medida que se crean entidades.\n","*   **Celda 9: Ejecutar An√°lisis del Sistema y Mostrar Reporte**: Llama a la funci√≥n `analizar_sistema`, pas√°ndole el directorio base y la ruta de configuraci√≥n. Esta funci√≥n realiza verificaciones sobre la estructura de directorios, la existencia de archivos clave, las estad√≠sticas de los datos procesados (si est√°n disponibles) y las estad√≠sticas del modelo sem√°ntico (si el Ontosistema fue inicializado). Genera un reporte de an√°lisis en `logs_sistema/reporte_analisis.txt` y luego el contenido de este reporte se imprime en la salida del notebook.\n","\n","Este flujo secuencial permite que el sistema avance paso a paso, desde la preparaci√≥n del entorno y la carga de datos, pasando por el procesamiento y el aprendizaje autom√°tico, hasta la construcci√≥n del modelo sem√°ntico y la generaci√≥n de un reporte de estado general.\n","\n","## 8. Funcionalidades Clave\n","\n","Basado en la estructura y los componentes implementados, el Sistema Fenomenol√≥gico Estructural v2.3 ofrece las siguientes capacidades clave:\n","\n","*   **Procesamiento y Extracci√≥n de Caracter√≠sticas**: Carga y limpia textos de entrada, transform√°ndolos en representaciones num√©ricas (vectores TF-IDF) adecuadas para an√°lisis computacional.\n","*   **Clasificaci√≥n Fenomenol√≥gica**: Entrena un modelo de aprendizaje autom√°tico (Random Forest) para clasificar autom√°ticamente los textos procesados en categor√≠as fenomenol√≥gicas predefinidas (percepci√≥n, emoci√≥n, cognici√≥n, etc.).\n","*   **Creaci√≥n y Gesti√≥n de Entidades Sem√°nticas**: Permite instanciar y describir formalmente diferentes tipos de entidades del modelo sem√°ntico (fen√≥menos, contextos, macrocontextos, metacampos) con atributos estructurados.\n","*   **Persistencia del Modelo Sem√°ntico**: Guarda las entidades del modelo sem√°ntico en archivos YAML, permitiendo la persistencia y recuperaci√≥n del estado del modelo entre sesiones o ejecuciones.\n","*   **Representaci√≥n de Relaciones Gr√°ficas**: Construye y gestiona redes de relaciones entre contextos y otras entidades sem√°nticas utilizando estructuras de grafo (NetworkX), facilitando el an√°lisis de las conexiones y la estructura subyacente del modelo.\n","*   **Seguimiento de Estad√≠sticas del Modelo Sem√°ntico**: El Ontosistema integrado rastrea m√©tricas cuantitativas sobre el contenido del modelo sem√°ntico (n√∫mero de entidades por tipo, apariciones del 'YO'), proporcionando una visi√≥n general de su tama√±o y composici√≥n.\n","*   **An√°lisis del Estado del Sistema y Reporte**: La funci√≥n `analizador_sistema` verifica la integridad de la estructura de directorios, la presencia de archivos clave, y resume las estad√≠sticas de los datos procesados y del modelo sem√°ntico, generando un reporte de estado consolidado para facilitar la depuraci√≥n y monitoreo.\n","*   **Comportamiento Configurable**: Permite ajustar par√°metros clave del procesamiento, clasificaci√≥n y gesti√≥n del modelo sem√°ntico a trav√©s de un archivo de configuraci√≥n (`config.yaml`) centralizado, ofreciendo flexibilidad sin modificar el c√≥digo base.\n","\n","## 9. Posibles Extensiones/Mejoras\n","\n","El Sistema Fenomenol√≥gico Estructural v2.3 establece una base s√≥lida para el an√°lisis fenomenol√≥gico, pero existen varias √°reas para futuras extensiones y mejoras que podr√≠an potenciar significativamente sus capacidades:\n","\n","*   **Preprocesamiento Avanzado**: Implementar t√©cnicas de preprocesamiento m√°s sofisticadas como la lematizaci√≥n o el an√°lisis de dependencias para capturar mejor el significado sint√°ctico y morfol√≥gico del texto.\n","*   **Extracci√≥n de Caracter√≠sticas con Embeddings/Transformers**: Explorar la extracci√≥n de caracter√≠sticas utilizando modelos de lenguaje pre-entrenados (como Word Embeddings, FastText, GloVe) o modelos basados en Transformers (como BERT, RoBERTa, etc.) para capturar relaciones sem√°nticas m√°s ricas y contextuales que TF-IDF.\n","*   **Algoritmos de Clasificaci√≥n Alternativos y Multi-etiqueta**: Evaluar y posiblemente integrar otros algoritmos de clasificaci√≥n (ej. SVM, redes neuronales, modelos basados en embeddings). Considerar la implementaci√≥n de clasificaci√≥n multi-etiqueta si un mismo texto puede pertenecer a m√∫ltiples categor√≠as fenomenol√≥gicas simult√°neamente.\n","*   **Extracci√≥n y Descubrimiento Automatizado en el Modelo Sem√°ntico**: Desarrollar m√≥dulos que identifiquen y extraigan autom√°ticamente conceptos clave, relaciones y patrones dentro de los textos para poblar el modelo sem√°ntico de forma menos manual. Investigar t√©cnicas de descubrimiento de relaciones basadas en procesamiento de lenguaje natural.\n","*   **Integraci√≥n Completa con Ontolog√≠as**: M√°s all√° de las categor√≠as principales, integrar el sistema con ontolog√≠as existentes o desarrollar una ontolog√≠a espec√≠fica para la fenomenolog√≠a que permita una representaci√≥n del conocimiento m√°s formal y extensible, facilitando inferencias y razonamientos complejos.\n","*   **Implementaci√≥n de Neo4j**: Activar y completar la implementaci√≥n de la integraci√≥n con la base de datos de grafos Neo4j (ya definida en `config.yaml`) para almacenar y consultar la `RedContextos` de manera m√°s eficiente y escalable, aprovechando las capacidades nativas de las bases de datos de grafos para el an√°lisis de redes.\n","*   **Interfaz de Usuario y Visualizaci√≥n**: Desarrollar una interfaz gr√°fica (ya sea web con Streamlit/Dash o de escritorio) para facilitar la interacci√≥n con el sistema, la carga de datos, la configuraci√≥n y, crucialmente, la visualizaci√≥n interactiva del modelo sem√°ntico (grafos de contextos, metacampos, etc.).\n","*   **Mejora del Proceso de Etiquetado**: Incorporar estrategias como el aprendizaje activo (active learning) o m√©todos semi-supervisados para ayudar a reducir la carga de etiquetado manual, permitiendo que el modelo aprenda de menos ejemplos anotados.\n","*   **Manejo de Errores y Logging Robusto**: Mejorar el manejo de excepciones y a√±adir un sistema de logging m√°s detallado para facilitar la depuraci√≥n y el monitoreo del sistema en producci√≥n.\n","*   **An√°lisis Profundo y M√©tricas**: Ampliar las capacidades de an√°lisis del `analizador_sistema` para incluir m√©tricas m√°s detalladas del modelo sem√°ntico (ej. medidas de centralidad en la red de contextos, an√°lisis temporal de fen√≥menos) y generar reportes m√°s interactivos o visuales.\n","\"\"\"\n","\n","# Asegurarse de que Google Drive est√© montado\n","try:\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"Google Drive montado.\")\n","except Exception as e:\n","    print(f\"Error al montar Google Drive: {e}\")\n","\n","\n","# Escribir el contenido en el archivo Markdown\n","try:\n","    with open(report_path_in_drive, 'w', encoding='utf-8') as f:\n","        f.write(report_content)\n","    print(f\"Reporte t√©cnico guardado exitosamente en: {report_path_in_drive}\")\n","except Exception as e:\n","    print(f\"Error al escribir el reporte en Google Drive: {e}\")"],"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive montado.\n","Reporte t√©cnico guardado exitosamente en: /content/drive/My Drive/reporte_tecnico_sistema_fenomenologico.md\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}