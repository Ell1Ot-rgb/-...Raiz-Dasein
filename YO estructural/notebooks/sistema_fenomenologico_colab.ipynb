{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Sistema Fenomenol√≥gico Estructural v2.2\n",
    "\n",
    "Este notebook procesa textos fenomenol√≥gicos y genera an√°lisis detallados usando el sistema YO emergente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias necesarias\n",
    "!pip install pandas numpy scikit-learn pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7024ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crear directorios principales\n",
    "os.makedirs('configuracion', exist_ok=True)\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "os.makedirs('scripts/modelos', exist_ok=True)\n",
    "os.makedirs('entrada_bruta', exist_ok=True)\n",
    "os.makedirs('features_extraidas', exist_ok=True)\n",
    "os.makedirs('logs_sistema', exist_ok=True)\n",
    "os.makedirs('procesado/nodos_fenomenologicos/v1/fenomenos', exist_ok=True)\n",
    "os.makedirs('procesado/nodos_fenomenologicos/v1/contextos', exist_ok=True)\n",
    "os.makedirs('procesado/nodos_fenomenologicos/v1/macrocontextos', exist_ok=True)\n",
    "os.makedirs('procesado/nodos_fenomenologicos/v1/redes', exist_ok=True)\n",
    "os.makedirs('procesado/nodos_fenomenologicos/v1/conceptos', exist_ok=True)\n",
    "os.makedirs('procesado/nodos_fenomenologicos/v1/metacampos', exist_ok=True)\n",
    "\n",
    "print(\"Directorios creados.\")\n",
    "\n",
    "# --- Archivo: configuracion/config.yaml ---\n",
    "config_content = \"\"\"\n",
    "procesamiento:\n",
    "  idioma: spanish\n",
    "  min_longitud_texto: 10\n",
    "  max_features_tfidf: 5000\n",
    "  ngram_range: [1, 2]\n",
    "  eliminar_stopwords: true\n",
    "\n",
    "clasificacion:\n",
    "  algoritmo: RandomForest\n",
    "  n_estimators: 100\n",
    "  test_size: 0.5 # Ajustado para evitar errores con pocos datos\n",
    "  random_state: 42\n",
    "\n",
    "fenomenologia:\n",
    "  categorias_base:\n",
    "    - experiencia_vivida\n",
    "    - conciencia_temporal\n",
    "    - intersubjetividad\n",
    "    - corporalidad\n",
    "    - espacialidad\n",
    "    - mundaneidad\n",
    "    - afectividad\n",
    "    - intencionalidad\n",
    "  umbral_relevancia: 0.6\n",
    "  generar_nodos_obsidian: true\n",
    "\n",
    "sistema:\n",
    "  backup_automatico: true\n",
    "  intervalo_backup_horas: 24\n",
    "  max_backups: 7\n",
    "  generar_reportes: true\n",
    "\n",
    "modelo_semantico:\n",
    "  rutas:\n",
    "    fenomenos: procesado/nodos_fenomenologicos/v1/fenomenos\n",
    "    contextos: procesado/nodos_fenomenologicos/v1/contextos\n",
    "    macrocontextos: procesado/nodos_fenomenologicos/v1/macrocontextos\n",
    "    redes: procesado/nodos_fenomenologicos/v1/redes\n",
    "    conceptos_emergentes: procesado/nodos_fenomenologicos/v1/conceptos\n",
    "    metacampos: procesado/nodos_fenomenologicos/v1/metacampos\n",
    "  umbrales:\n",
    "    agrupacion_contextos: 0.7\n",
    "    emergencia_yo: 0.6\n",
    "    creacion_concepto: 0.5\n",
    "  neo4j:\n",
    "    enabled: false\n",
    "    uri: bolt://localhost:7687\n",
    "    user: neo4j\n",
    "    password: password\n",
    "\"\"\"\n",
    "with open('configuracion/config.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(config_content)\n",
    "print(\"config.yaml creado.\")\n",
    "\n",
    "# --- Archivo: scripts/modelos/ontosistema.py ---\n",
    "ontosistema_py_content = \"\"\"\n",
    "import os\n",
    "import yaml\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "class Ontosistema:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.rutas = config.get('modelo_semantico', {}).get('rutas', {})\n",
    "        self.estadisticas = {\n",
    "            \"fenomenos\": 0,\n",
    "            \"contextos\": 0,\n",
    "            \"macrocontextos\": 0,\n",
    "            \"redes\": 0,\n",
    "            \"conceptos_emergentes\": 0,\n",
    "            \"apariciones_yo\": 0\n",
    "        }\n",
    "        self.ultima_actualizacion = datetime.datetime.now().isoformat()\n",
    "\n",
    "    def actualizar_estadisticas(self):\n",
    "        for tipo, ruta_relativa in self.rutas.items():\n",
    "            # En Colab, las rutas son relativas al directorio ra√≠z del notebook\n",
    "            ruta_completa = ruta_relativa \n",
    "            if os.path.exists(ruta_completa):\n",
    "                archivos = [f for f in os.listdir(ruta_completa) if f.endswith('.yaml')]\n",
    "                self.estadisticas[tipo] = len(archivos)\n",
    "            else:\n",
    "                # Si la ruta no existe, podr√≠a ser la primera ejecuci√≥n, inicializar a 0\n",
    "                self.estadisticas[tipo] = 0\n",
    "                # print(f\"Advertencia: La ruta {ruta_completa} para {tipo} no existe.\") # Opcional: para depuraci√≥n\n",
    "\n",
    "        self.contar_apariciones_yo()\n",
    "        self.ultima_actualizacion = datetime.datetime.now().isoformat()\n",
    "        return self.estadisticas\n",
    "\n",
    "    def contar_apariciones_yo(self):\n",
    "        ruta_contextos_relativa = self.rutas.get('contextos', '')\n",
    "        # En Colab, las rutas son relativas al directorio ra√≠z del notebook\n",
    "        ruta_contextos = ruta_contextos_relativa \n",
    "        if os.path.exists(ruta_contextos):\n",
    "            contador = 0\n",
    "            for archivo in os.listdir(ruta_contextos):\n",
    "                if archivo.endswith('.yaml'):\n",
    "                    try:\n",
    "                        with open(os.path.join(ruta_contextos, archivo), 'r', encoding='utf-8') as f:\n",
    "                            data = yaml.safe_load(f)\n",
    "                            if data.get('yo_presente', False):\n",
    "                                contador += 1\n",
    "                    except Exception:\n",
    "                        pass # Ignorar errores de lectura de archivos individuales\n",
    "            self.estadisticas[\"apariciones_yo\"] = contador\n",
    "        else:\n",
    "            self.estadisticas[\"apariciones_yo\"] = 0\n",
    "            # print(f\"Advertencia: La ruta de contextos {ruta_contextos} no existe.\") # Opcional\n",
    "\n",
    "    def guardar_estadisticas(self, ruta_logs):\n",
    "        os.makedirs(ruta_logs, exist_ok=True) # Asegurar que el directorio de logs exista\n",
    "        ruta_completa = os.path.join(ruta_logs, 'metricas.json')\n",
    "        datos = {\n",
    "            \"estadisticas\": self.estadisticas,\n",
    "            \"timestamp\": self.ultima_actualizacion\n",
    "        }\n",
    "        with open(ruta_completa, 'w', encoding='utf-8') as f:\n",
    "            json.dump(datos, f, indent=2, ensure_ascii=False)\n",
    "        return ruta_completa\n",
    "\n",
    "    def cargar_estadisticas(self, ruta_logs):\n",
    "        ruta_completa = os.path.join(ruta_logs, 'metricas.json')\n",
    "        if os.path.exists(ruta_completa):\n",
    "            with open(ruta_completa, 'r', encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                self.estadisticas = datos.get(\"estadisticas\", self.estadisticas)\n",
    "                self.ultima_actualizacion = datos.get(\"timestamp\", self.ultima_actualizacion)\n",
    "        return self.estadisticas\n",
    "\"\"\"\n",
    "with open('scripts/modelos/ontosistema.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(ontosistema_py_content)\n",
    "print(\"scripts/modelos/ontosistema.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/modelos/fenomeno.py (Ejemplo b√°sico, necesitar√°s el tuyo) ---\n",
    "fenomeno_py_content = \"\"\"\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "class Fenomeno:\n",
    "    def __init__(self, contenido, tipo=\"general\", propiedades=None):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.contenido = contenido\n",
    "        self.tipo = tipo\n",
    "        self.propiedades = propiedades or {}\n",
    "        self.timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\"\"\"\n",
    "with open('scripts/modelos/fenomeno.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(fenomeno_py_content)\n",
    "print(\"scripts/modelos/fenomeno.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/modelos/contexto.py (Ejemplo b√°sico) ---\n",
    "contexto_py_content = \"\"\"\n",
    "import uuid\n",
    "import datetime\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "class Contexto:\n",
    "    def __init__(self, descripcion=\"\"):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.descripcion = descripcion\n",
    "        self.fenomenos_ids = []\n",
    "        self.flujos = []\n",
    "        self.yo_presente = False\n",
    "        self.proyeccion = \"\"\n",
    "        self.timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "    def agregar_fenomeno(self, fenomeno_obj):\n",
    "        if hasattr(fenomeno_obj, 'id'):\n",
    "            self.fenomenos_ids.append(fenomeno_obj.id)\n",
    "        else: # Asumir que es un ID si no es un objeto\n",
    "            self.fenomenos_ids.append(fenomeno_obj)\n",
    "\n",
    "    def agregar_flujo(self, flujo_descripcion):\n",
    "        self.flujos.append(flujo_descripcion)\n",
    "\n",
    "    def activar_yo(self):\n",
    "        self.yo_presente = True\n",
    "\n",
    "    def establecer_proyeccion(self, proyeccion_texto):\n",
    "        self.proyeccion = proyeccion_texto\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def guardar(self, ruta_base):\n",
    "        os.makedirs(ruta_base, exist_ok=True)\n",
    "        ruta_completa = os.path.join(ruta_base, f\"contexto_{self.id}.yaml\")\n",
    "        with open(ruta_completa, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(self.to_dict(), f, default_flow_style=False, allow_unicode=True)\n",
    "        return ruta_completa\n",
    "\"\"\"\n",
    "with open('scripts/modelos/contexto.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(contexto_py_content)\n",
    "print(\"scripts/modelos/contexto.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/modelos/macrocontexto.py (Ejemplo b√°sico) ---\n",
    "macrocontexto_py_content = \"\"\"\n",
    "import uuid\n",
    "import datetime\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "class Macrocontexto:\n",
    "    def __init__(self, nombre, descripcion=\"\"):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.nombre = nombre\n",
    "        self.descripcion = descripcion\n",
    "        self.contextos_ids = []\n",
    "        self.patron = \"\"\n",
    "        self.reflexion = \"\"\n",
    "        self.timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "    def agregar_contexto(self, contexto_id):\n",
    "        self.contextos_ids.append(contexto_id)\n",
    "\n",
    "    def establecer_patron(self, patron_texto):\n",
    "        self.patron = patron_texto\n",
    "\n",
    "    def establecer_reflexion(self, reflexion_texto):\n",
    "        self.reflexion = reflexion_texto\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def guardar(self, ruta_base):\n",
    "        os.makedirs(ruta_base, exist_ok=True)\n",
    "        ruta_completa = os.path.join(ruta_base, f\"macrocontexto_{self.id}.yaml\")\n",
    "        with open(ruta_completa, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(self.to_dict(), f, default_flow_style=False, allow_unicode=True)\n",
    "        return ruta_completa\n",
    "\"\"\"\n",
    "with open('scripts/modelos/macrocontexto.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(macrocontexto_py_content)\n",
    "print(\"scripts/modelos/macrocontexto.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/modelos/red_contextos.py (Ejemplo b√°sico) ---\n",
    "red_contextos_py_content = \"\"\"\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "class RedContextos:\n",
    "    def __init__(self, nombre=\"Red por defecto\"):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.nombre = nombre\n",
    "        self.nodos = [] # IDs de contextos\n",
    "        self.aristas = [] # Tuplas (id_origen, id_destino, tipo_relacion)\n",
    "        self.timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\"\"\"\n",
    "with open('scripts/modelos/red_contextos.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(red_contextos_py_content)\n",
    "print(\"scripts/modelos/red_contextos.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/modelos/metacampo.py (Ejemplo b√°sico) ---\n",
    "metacampo_py_content = \"\"\"\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "class Metacampo:\n",
    "    def __init__(self, nombre, descripcion=\"\"):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.nombre = nombre\n",
    "        self.descripcion = descripcion\n",
    "        self.conceptos_asociados = [] # IDs de conceptos\n",
    "        self.timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\"\"\"\n",
    "with open('scripts/modelos/metacampo.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(metacampo_py_content)\n",
    "print(\"scripts/modelos/metacampo.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/extractor_features.py ---\n",
    "extractor_features_py_content = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import yaml\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar stopwords si no est√°n presentes (solo la primera vez)\n",
    "try:\n",
    "    stopwords.words('spanish')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "def cargar_config_extractor(): # Renombrado para evitar colisi√≥n\n",
    "    # En Colab, la ruta es relativa al directorio ra√≠z del notebook\n",
    "    ruta = 'configuracion/config.yaml'\n",
    "    with open(ruta, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "class ExtractorFeatures:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config if config else cargar_config_extractor()\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=self.config['procesamiento']['max_features_tfidf'],\n",
    "            ngram_range=tuple(self.config['procesamiento']['ngram_range']),\n",
    "            stop_words=stopwords.words(self.config['procesamiento']['idioma']) if self.config['procesamiento']['eliminar_stopwords'] else None\n",
    "        )\n",
    "\n",
    "    def limpiar_texto(self, texto):\n",
    "        # Implementa tu l√≥gica de limpieza aqu√≠ si es necesario\n",
    "        return texto\n",
    "\n",
    "    def extraer_features_tfidf(self, textos_crudos):\n",
    "        textos_limpios = [self.limpiar_texto(texto) for texto in textos_crudos]\n",
    "        if not textos_limpios: # Si no hay textos, devuelve un DataFrame vac√≠o\n",
    "            return pd.DataFrame()\n",
    "        features = self.vectorizer.fit_transform(textos_limpios)\n",
    "        df_features = pd.DataFrame(features.toarray(), columns=self.vectorizer.get_feature_names_out())\n",
    "        return df_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejemplo de uso (adaptado para Colab)\n",
    "    config_main = cargar_config_extractor()\n",
    "    extractor = ExtractorFeatures(config_main)\n",
    "    \n",
    "    # Crear archivos de ejemplo en entrada_bruta para la prueba\n",
    "    os.makedirs('entrada_bruta', exist_ok=True)\n",
    "    textos_ejemplo = [\n",
    "        \"Este es el primer texto fenomenol√≥gico sobre la experiencia vivida.\",\n",
    "        \"Un segundo relato acerca de la conciencia temporal y el ser.\",\n",
    "        \"La corporalidad se manifiesta en cada movimiento y sensaci√≥n.\",\n",
    "        \"Explorando la mundaneidad de los objetos cotidianos.\",\n",
    "        \"Otro texto m√°s para tener suficientes datos.\",\n",
    "        \"La percepci√≥n del tiempo es fundamental.\",\n",
    "        \"Siento mi cuerpo en el espacio.\",\n",
    "        \"Los objetos me rodean en mi mundo.\"\n",
    "    ]\n",
    "    for i, texto in enumerate(textos_ejemplo):\n",
    "        with open(f'entrada_bruta/texto_fenomenologico_{i+1}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(texto)\n",
    "\n",
    "    # Cargar textos desde los archivos creados\n",
    "    textos_cargados = []\n",
    "    for i in range(1, 9):\n",
    "        with open(f'entrada_bruta/texto_fenomenologico_{i}.txt', 'r', encoding='utf-8') as f:\n",
    "            textos_cargados.append(f.read())\n",
    "\n",
    "    if textos_cargados:\n",
    "        df_features = extractor.extraer_features_tfidf(textos_cargados)\n",
    "        print(\"Caracter√≠sticas extra√≠das:\")\n",
    "        print(df_features.head())\n",
    "        \n",
    "        # Guardar features y etiquetas (ejemplo)\n",
    "        os.makedirs('features_extraidas', exist_ok=True)\n",
    "        df_features.to_csv('features_extraidas/features_tfidf.csv', index=False)\n",
    "        print(\"features_tfidf.csv guardado.\")\n",
    "        \n",
    "        # Crear archivo de etiquetas de ejemplo\n",
    "        etiquetas_ejemplo = [\n",
    "            'experiencia_vivida',\n",
    "            'conciencia_temporal',\n",
    "            'corporalidad',\n",
    "            'mundaneidad',\n",
    "            'experiencia_vivida', # Asegurar al menos 2 por clase para test_size=0.5\n",
    "            'conciencia_temporal',\n",
    "            'corporalidad',\n",
    "            'mundaneidad'\n",
    "        ]\n",
    "        df_etiquetas = pd.DataFrame(etiquetas_ejemplo, columns=['clase'])\n",
    "        df_etiquetas.to_csv('features_extraidas/etiquetas.csv', index=False)\n",
    "        print(\"etiquetas.csv guardado.\")\n",
    "    else:\n",
    "        print(\"No se cargaron textos para extraer caracter√≠sticas.\")\n",
    "\"\"\"\n",
    "with open('scripts/extractor_features.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(extractor_features_py_content)\n",
    "print(\"scripts/extractor_features.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/clasificador.py ---\n",
    "clasificador_py_content = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "def cargar_config_clasificador(): # Renombrado para evitar colisi√≥n\n",
    "    ruta = 'configuracion/config.yaml'\n",
    "    with open(ruta, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "class ClasificadorFenomenologico:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.modelo = RandomForestClassifier(\n",
    "            n_estimators=config['clasificacion']['n_estimators'],\n",
    "            random_state=config['clasificacion']['random_state'],\n",
    "            n_jobs=-1 # Usar todos los procesadores disponibles\n",
    "        )\n",
    "\n",
    "    def entrenar(self, X, y):\n",
    "        if len(X) == 0 or len(y) == 0:\n",
    "            print(\"No hay datos para entrenar.\")\n",
    "            return\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(f\"El n√∫mero de muestras no coincide: {len(X)} features vs {len(y)} etiquetas\")\n",
    "        \n",
    "        # Verificar si hay suficientes muestras por clase para stratify\n",
    "        from collections import Counter\n",
    "        conteo_clases = Counter(y)\n",
    "        clases_con_una_muestra = [clase for clase, conteo in conteo_clases.items() if conteo < 2]\n",
    "        \n",
    "        stratify_param = y\n",
    "        if clases_con_una_muestra and self.config['clasificacion']['test_size'] > 0:\n",
    "            print(f\"Advertencia: Las siguientes clases tienen solo 1 muestra y test_size > 0: {clases_con_una_muestra}. No se usar√° 'stratify'.\")\n",
    "            stratify_param = None\n",
    "            \n",
    "        # Asegurar que test_size no sea mayor que el n√∫mero de muestras de la clase m√°s peque√±a si stratify est√° activo\n",
    "        min_samples_in_class = min(conteo_clases.values()) if conteo_clases else 0\n",
    "        current_test_size = self.config['clasificacion']['test_size']\n",
    "\n",
    "        if stratify_param is not None and min_samples_in_class > 0 and int(min_samples_in_class * current_test_size) < 1:\n",
    "            # Si el test_size resultar√≠a en 0 muestras de prueba para la clase m√°s peque√±a, ajustar o advertir\n",
    "            # Por simplicidad, aqu√≠ simplemente no usamos stratify si podr√≠a causar problemas.\n",
    "            # Una mejor soluci√≥n ser√≠a ajustar test_size o manejarlo de forma m√°s robusta.\n",
    "            print(f\"Advertencia: test_size ({current_test_size}) es demasiado grande para la clase m√°s peque√±a ({min_samples_in_class} muestras). No se usar√° 'stratify'.\")\n",
    "            stratify_param = None\n",
    "\n",
    "        if len(set(y)) < 2 and current_test_size > 0:\n",
    "             print(\"Advertencia: Solo hay una clase presente. El conjunto de prueba estar√° vac√≠o o el entrenamiento fallar√° si test_size > 0.\")\n",
    "             # No se puede dividir si solo hay una clase y test_size > 0\n",
    "             X_train, X_test, y_train, y_test = X, pd.DataFrame(), y, [] # O manejar de otra forma\n",
    "        elif current_test_size == 0: # Si test_size es 0, usar todos los datos para entrenar\n",
    "            X_train, X_test, y_train, y_test = X, pd.DataFrame(), y, []\n",
    "        else:\n",
    "            try:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y,\n",
    "                    test_size=current_test_size,\n",
    "                    random_state=self.config['clasificacion']['random_state'],\n",
    "                    stratify=stratify_param\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                 print(f\"Error durante train_test_split (probablemente debido a pocas muestras por clase con stratify): {e}. Intentando sin stratify.\")\n",
    "                 X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y,\n",
    "                    test_size=current_test_size,\n",
    "                    random_state=self.config['clasificacion']['random_state'],\n",
    "                    stratify=None # Intentar sin stratify\n",
    "                )\n",
    "\n",
    "        if not X_train.empty if isinstance(X_train, pd.DataFrame) else len(X_train) > 0:\n",
    "            self.modelo.fit(X_train, y_train)\n",
    "            if not X_test.empty if isinstance(X_test, pd.DataFrame) else len(X_test) > 0:\n",
    "                score = self.modelo.score(X_test, y_test)\n",
    "                print(f\"Precisi√≥n en test: {score:.2f}\")\n",
    "            else:\n",
    "                print(\"El conjunto de test est√° vac√≠o, no se calcula la precisi√≥n.\")\n",
    "        else:\n",
    "            print(\"El conjunto de entrenamiento est√° vac√≠o.\")\n",
    "\n",
    "    def predecir(self, X):\n",
    "        if not hasattr(self.modelo, 'classes_'):\n",
    "            print(\"El modelo no ha sido entrenado a√∫n.\")\n",
    "            return []\n",
    "        return self.modelo.predict(X)\n",
    "\n",
    "    def predecir_probabilidades(self, X):\n",
    "        if not hasattr(self.modelo, 'classes_'):\n",
    "            print(\"El modelo no ha sido entrenado a√∫n.\")\n",
    "            return []\n",
    "        return self.modelo.predict_proba(X)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_main = cargar_config_clasificador()\n",
    "    \n",
    "    # Cargar features (aseg√∫rate que extractor_features.py se haya ejecutado)\n",
    "    features_path = 'features_extraidas/features_tfidf.csv'\n",
    "    etiquetas_path = 'features_extraidas/etiquetas.csv'\n",
    "\n",
    "    if os.path.exists(features_path) and os.path.exists(etiquetas_path):\n",
    "        df = pd.read_csv(features_path)\n",
    "        etiquetas_df = pd.read_csv(etiquetas_path)\n",
    "        y = etiquetas_df['clase'].tolist()\n",
    "        \n",
    "        if len(df) != len(y):\n",
    "            raise ValueError(f\"N√∫mero de muestras no coincide: {len(df)} features vs {len(y)} etiquetas\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"El DataFrame de caracter√≠sticas est√° vac√≠o.\")\n",
    "        else:\n",
    "            clf = ClasificadorFenomenologico(config_main)\n",
    "            clf.entrenar(df, y) # df ya son los valores num√©ricos\n",
    "            # Ejemplo de predicci√≥n\n",
    "            if hasattr(clf.modelo, 'classes_'): # Verificar si el modelo fue entrenado\n",
    "                 predicciones = clf.predecir(df)\n",
    "                 print(f\"Predicciones de ejemplo: {predicciones[:5]}\")\n",
    "                 probabilidades = clf.predecir_probabilidades(df)\n",
    "                 print(f\"Probabilidades de ejemplo: {probabilidades[:5]}\")\n",
    "    else:\n",
    "        print(f\"Aseg√∫rate que '{features_path}' y '{etiquetas_path}' existan. Ejecuta extractor_features.py primero.\")\n",
    "\"\"\"\n",
    "with open('scripts/clasificador.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(clasificador_py_content)\n",
    "print(\"scripts/clasificador.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/gestor_modelo_semantico.py ---\n",
    "gestor_modelo_semantico_py_content = \"\"\"\n",
    "import os\n",
    "import yaml\n",
    "import datetime\n",
    "# Aseg√∫rate que las rutas de importaci√≥n sean correctas para Colab\n",
    "from scripts.modelos.fenomeno import Fenomeno\n",
    "from scripts.modelos.contexto import Contexto\n",
    "from scripts.modelos.macrocontexto import Macrocontexto\n",
    "from scripts.modelos.red_contextos import RedContextos\n",
    "from scripts.modelos.ontosistema import Ontosistema\n",
    "from scripts.modelos.metacampo import Metacampo\n",
    "\n",
    "def cargar_config_gestor(): # Renombrado\n",
    "    ruta = 'configuracion/config.yaml'\n",
    "    with open(ruta, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "class GestorModeloSemantico:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or cargar_config_gestor()\n",
    "        self.inicializar_config_modelo() # Asegura que la secci√≥n 'modelo_semantico' exista\n",
    "        self.ontosistema = Ontosistema(self.config)\n",
    "        self.crear_directorios_modelo() # Renombrado para claridad\n",
    "    \n",
    "    def inicializar_config_modelo(self):\n",
    "        if 'modelo_semantico' not in self.config:\n",
    "            self.config['modelo_semantico'] = {}\n",
    "        if 'rutas' not in self.config['modelo_semantico']:\n",
    "            self.config['modelo_semantico']['rutas'] = {\n",
    "                'fenomenos': 'procesado/nodos_fenomenologicos/v1/fenomenos',\n",
    "                'contextos': 'procesado/nodos_fenomenologicos/v1/contextos',\n",
    "                'macrocontextos': 'procesado/nodos_fenomenologicos/v1/macrocontextos',\n",
    "                'redes': 'procesado/nodos_fenomenologicos/v1/redes',\n",
    "                'conceptos_emergentes': 'procesado/nodos_fenomenologicos/v1/conceptos',\n",
    "                'metacampos': 'procesado/nodos_fenomenologicos/v1/metacampos'\n",
    "            }\n",
    "        # Podr√≠as a√±adir m√°s inicializaciones de umbrales, neo4j, etc. si es necesario\n",
    "        # self.guardar_config_gestor() # Opcional: guardar si se modific√≥\n",
    "\n",
    "    def guardar_config_gestor(self): # Renombrado\n",
    "        ruta = 'configuracion/config.yaml'\n",
    "        with open(ruta, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "    def crear_directorios_modelo(self): # Renombrado\n",
    "        if 'rutas' in self.config.get('modelo_semantico', {}):\n",
    "            for tipo, ruta_relativa in self.config['modelo_semantico']['rutas'].items():\n",
    "                # En Colab, las rutas son relativas al directorio ra√≠z del notebook\n",
    "                os.makedirs(ruta_relativa, exist_ok=True)\n",
    "        else:\n",
    "            print(\"Advertencia: No se encontraron rutas en la configuraci√≥n del modelo sem√°ntico para crear directorios.\")\n",
    "\n",
    "    def crear_fenomeno(self, contenido, tipo=\"general\", propiedades=None):\n",
    "        fenomeno = Fenomeno(contenido, tipo, propiedades)\n",
    "        ruta_base_relativa = self.config['modelo_semantico']['rutas']['fenomenos']\n",
    "        # ruta_base = os.path.join(os.path.dirname(__file__), '..', ruta_base_relativa) # Ajuste para Colab\n",
    "        ruta_base = ruta_base_relativa\n",
    "        os.makedirs(ruta_base, exist_ok=True)\n",
    "        ruta_completa = os.path.join(ruta_base, f\"fenomeno_{fenomeno.id}.yaml\")\n",
    "        with open(ruta_completa, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(fenomeno.to_dict(), f, default_flow_style=False, allow_unicode=True)\n",
    "        return fenomeno\n",
    "\n",
    "    def crear_contexto(self, descripcion=\"\", fenomenos=None, flujos=None, yo_presente=False, proyeccion=\"\"):\n",
    "        contexto = Contexto(descripcion)\n",
    "        if fenomenos: [contexto.agregar_fenomeno(f) for f in fenomenos]\n",
    "        if flujos: [contexto.agregar_flujo(flujo) for flujo in flujos]\n",
    "        if yo_presente: contexto.activar_yo()\n",
    "        if proyeccion: contexto.establecer_proyeccion(proyeccion)\n",
    "        \n",
    "        ruta_base_relativa = self.config['modelo_semantico']['rutas']['contextos']\n",
    "        # ruta_base = os.path.join(os.path.dirname(__file__), '..', ruta_base_relativa) # Ajuste para Colab\n",
    "        ruta_base = ruta_base_relativa\n",
    "        contexto.guardar(ruta_base)\n",
    "        return contexto\n",
    "\n",
    "    def crear_macrocontexto(self, nombre, descripcion, contextos_ids, patron=\"\", reflexion=\"\"):\n",
    "        macrocontexto = Macrocontexto(nombre, descripcion)\n",
    "        for ctx_id in contextos_ids: macrocontexto.agregar_contexto(ctx_id)\n",
    "        if patron: macrocontexto.establecer_patron(patron)\n",
    "        if reflexion: macrocontexto.establecer_reflexion(reflexion)\n",
    "        \n",
    "        ruta_base_relativa = self.config['modelo_semantico']['rutas']['macrocontextos']\n",
    "        # ruta_base = os.path.join(os.path.dirname(__file__), '..', ruta_base_relativa) # Ajuste para Colab\n",
    "        ruta_base = ruta_base_relativa\n",
    "        macrocontexto.guardar(ruta_base)\n",
    "        return macrocontexto\n",
    "\n",
    "    def actualizar_estadisticas(self):\n",
    "        self.ontosistema.actualizar_estadisticas()\n",
    "        # ruta_logs = os.path.join(os.path.dirname(__file__), '..', 'logs_sistema') # Ajuste para Colab\n",
    "        ruta_logs = 'logs_sistema'\n",
    "        self.ontosistema.guardar_estadisticas(ruta_logs)\n",
    "        return self.ontosistema.estadisticas\n",
    "    \n",
    "    def obtener_estado(self): # M√©todo a√±adido en analizador_local.py\n",
    "        # Simula la obtenci√≥n de un estado, puedes hacerlo m√°s complejo\n",
    "        stats = self.ontosistema.estadisticas\n",
    "        return {\n",
    "            \"total_conceptos\": stats.get(\"conceptos_emergentes\", 0) + stats.get(\"fenomenos\", 0), # Ejemplo\n",
    "            \"total_relaciones\": stats.get(\"contextos\", 0) + stats.get(\"redes\",0) # Ejemplo\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gestor = GestorModeloSemantico()\n",
    "    # gestor.crear_directorios_modelo() # Ya se llama en __init__\n",
    "    \n",
    "    fenomeno1 = gestor.crear_fenomeno(\"Sensaci√≥n de Colab\", \"digital\")\n",
    "    contexto1 = gestor.crear_contexto(\"Usando Colab\", fenomenos=[fenomeno1], yo_presente=True)\n",
    "    \n",
    "    stats = gestor.actualizar_estadisticas()\n",
    "    print(\"Estad√≠sticas del gestor actualizadas:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "\"\"\"\n",
    "with open('scripts/gestor_modelo_semantico.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(gestor_modelo_semantico_py_content)\n",
    "print(\"scripts/gestor_modelo_semantico.py creado.\")\n",
    "\n",
    "# --- Archivo: scripts/analizador_sistema.py ---\n",
    "analizador_sistema_py_content = \"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml # Asegurar importaci√≥n\n",
    "from collections import Counter # Asegurar importaci√≥n\n",
    "\n",
    "# Ajustar rutas de importaci√≥n para Colab\n",
    "from scripts.clasificador import ClasificadorFenomenologico #, cargar_config_clasificador (usar una sola func cargar_config)\n",
    "from scripts.gestor_modelo_semantico import GestorModeloSemantico\n",
    "\n",
    "# Funci√≥n unificada para cargar config, ya que todas las versiones son iguales\n",
    "def cargar_config_analizador(): \n",
    "    ruta = 'configuracion/config.yaml'\n",
    "    with open(ruta, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def analizar_sistema():\n",
    "    # ruta_base_script = os.path.dirname(__file__) # No aplicable directamente en Colab para __file__\n",
    "    # reporte_path = os.path.join(ruta_base_script, '..', 'logs_sistema', 'reporte_analisis.txt')\n",
    "    reporte_path = 'logs_sistema/reporte_analisis.txt'\n",
    "    os.makedirs('logs_sistema', exist_ok=True)\n",
    "\n",
    "    with open(reporte_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== REPORTE DE AN√ÅLISIS DEL SISTEMA (COLAB) ===\\n\\n\")\n",
    "        \n",
    "        directorios = ['entrada_bruta', 'features_extraidas', 'logs_sistema'] # 'clasificados' no se usa activamente aqu√≠\n",
    "        f.write(\"1. Verificaci√≥n de directorios:\\n\")\n",
    "        for dir_name in directorios:\n",
    "            # path = os.path.join(ruta_base_script, '..', dir_name)\n",
    "            path = dir_name # Rutas relativas en Colab\n",
    "            existe = os.path.exists(path)\n",
    "            f.write(f\"   - {dir_name}: {'‚úì' if existe else 'X'}\\n\")\n",
    "        \n",
    "        f.write(\"\\n2. Verificaci√≥n de archivos:\\n\")\n",
    "        archivos_necesarios = {\n",
    "            'features_tfidf.csv': 'features_extraidas/features_tfidf.csv',\n",
    "            'etiquetas.csv': 'features_extraidas/etiquetas.csv',\n",
    "            'config.yaml': 'configuracion/config.yaml'\n",
    "        }\n",
    "        for nombre, ruta_relativa in archivos_necesarios.items():\n",
    "            # path = os.path.join(ruta_base_script, '..', ruta_relativa)\n",
    "            path = ruta_relativa # Rutas relativas en Colab\n",
    "            existe = os.path.exists(path)\n",
    "            f.write(f\"   - {nombre}: {'‚úì' if existe else 'X'}\\n\")\n",
    "        \n",
    "        try:\n",
    "            config = cargar_config_analizador()\n",
    "            \n",
    "            # Verificar si los archivos de datos existen antes de intentar leerlos\n",
    "            if not os.path.exists(archivos_necesarios['features_tfidf.csv']):\n",
    "                f.write(f\"\\nERROR: El archivo {archivos_necesarios['features_tfidf.csv']} no existe. Ejecuta la celda de extracci√≥n de features primero.\\n\")\n",
    "                df = pd.DataFrame() # DataFrame vac√≠o para evitar errores posteriores\n",
    "            else:\n",
    "                df = pd.read_csv(archivos_necesarios['features_tfidf.csv'])\n",
    "\n",
    "            if not os.path.exists(archivos_necesarios['etiquetas.csv']):\n",
    "                f.write(f\"\\nERROR: El archivo {archivos_necesarios['etiquetas.csv']} no existe. Ejecuta la celda de extracci√≥n de features primero.\\n\")\n",
    "                y = [] # Lista vac√≠a\n",
    "            else:\n",
    "                etiquetas_df = pd.read_csv(archivos_necesarios['etiquetas.csv'])\n",
    "                y = etiquetas_df['clase'].tolist()\n",
    "            \n",
    "            f.write(f\"\\n3. An√°lisis de datos:\\n\")\n",
    "            f.write(f\"   - N√∫mero de textos (features): {len(df)}\\n\")\n",
    "            f.write(f\"   - N√∫mero de features: {df.shape[1] if not df.empty else 0}\\n\")\n",
    "            f.write(f\"   - N√∫mero de etiquetas: {len(y)}\\n\")\n",
    "            f.write(f\"   - Categor√≠as √∫nicas: {len(set(y)) if y else 0}\\n\")\n",
    "            \n",
    "            conteo = Counter(y)\n",
    "            f.write(\"\\n4. Distribuci√≥n de categor√≠as:\\n\")\n",
    "            for categoria, count in conteo.items():\n",
    "                f.write(f\"   - {categoria}: {count} ejemplos\\n\")\n",
    "                if count < 2 and config['clasificacion']['test_size'] > 0:\n",
    "                    f.write(f\"     ¬°ADVERTENCIA! La categor√≠a {categoria} tiene menos de 2 ejemplos, puede afectar la divisi√≥n train/test.\\n\")\n",
    "            \n",
    "            f.write(\"\\n5. An√°lisis del modelo sem√°ntico:\\n\")\n",
    "            try:\n",
    "                gestor = GestorModeloSemantico(config)\n",
    "                if 'modelo_semantico' in config and 'rutas' in config['modelo_semantico']:\n",
    "                    for nivel, ruta_relativa_ms in config['modelo_semantico']['rutas'].items():\n",
    "                        # ruta_completa_ms = os.path.join(ruta_base_script, '..', ruta_relativa_ms)\n",
    "                        ruta_completa_ms = ruta_relativa_ms # Rutas relativas en Colab\n",
    "                        existe_ms = os.path.exists(ruta_completa_ms)\n",
    "                        f.write(f\"   - Directorio {nivel}: {'‚úì' if existe_ms else 'X'}\\n\")\n",
    "                else:\n",
    "                    f.write(\"   - No se encontr√≥ la configuraci√≥n de rutas del modelo sem√°ntico.\\n\")\n",
    "                \n",
    "                stats = gestor.actualizar_estadisticas() # Actualiza y obtiene las estad√≠sticas\n",
    "                f.write(\"\\n6. Estad√≠sticas del modelo sem√°ntico:\\n\")\n",
    "                for k, v_stat in stats.items(): f.write(f\"   - {k.capitalize()}: {v_stat}\\n\")\n",
    "                \n",
    "                f.write(\"\\n7. An√°lisis de escalabilidad:\\n\")\n",
    "                total_elementos = sum(stats.values())\n",
    "                f.write(f\"   - Total de elementos: {total_elementos}\\n\")\n",
    "                if total_elementos > 0 and stats.get('contextos', 0) > 0: # Evitar divisi√≥n por cero\n",
    "                    porcentaje_yo = (stats.get('apariciones_yo',0) / stats['contextos']) * 100 if stats['contextos'] > 0 else 0\n",
    "                    f.write(f\"   - Porcentaje de emergencia del YO (en contextos): {porcentaje_yo:.1f}%\\n\")\n",
    "                    if stats.get('redes',0) > 0:\n",
    "                        densidad = stats['contextos'] / stats['redes']\n",
    "                        f.write(f\"   - Densidad de red (contextos/red): {densidad:.1f}\\n\")\n",
    "                else:\n",
    "                    f.write(\"   - No hay suficientes elementos para calcular porcentaje de YO o densidad de red.\\n\")\n",
    "\n",
    "                f.write(\"\\n8. Verificaci√≥n de integridad (simplificada para Colab):\\n\")\n",
    "                # Esta verificaci√≥n es m√°s compleja de replicar fielmente sin la estructura de archivos local exacta.\n",
    "                # Se omite la comprobaci√≥n detallada de conteo de archivos vs stats para simplificar.\n",
    "                f.write(\"   - Integridad de datos: (Verificaci√≥n detallada omitida en Colab)\\n\")\n",
    "\n",
    "            except Exception as e_ms:\n",
    "                f.write(f\"   - ERROR en an√°lisis del modelo sem√°ntico: {str(e_ms)}\\n\")\n",
    "            \n",
    "            f.write(\"\\n9. Prueba de clasificaci√≥n:\\n\")\n",
    "            if not df.empty and y:\n",
    "                try:\n",
    "                    clasificador = ClasificadorFenomenologico(config)\n",
    "                    # X_values = df.values # No es necesario si df ya es num√©rico y sin index/columnas no deseadas\n",
    "                    \n",
    "                    if len(set(y)) < 2 and config['clasificacion']['test_size'] > 0:\n",
    "                        f.write(\"   - ERROR: Se necesitan al menos 2 categor√≠as diferentes para entrenar con un test_size > 0.\\n\")\n",
    "                    elif len(df) < 2 : # O alguna otra heur√≠stica para \"muy pocos datos\"\n",
    "                         f.write(\"   - ERROR: Muy pocos datos para realizar una divisi√≥n train/test significativa.\\n\")\n",
    "                    else:\n",
    "                        clasificador.entrenar(df, y) # Pasar el DataFrame directamente\n",
    "                        f.write(\"   - Entrenamiento: ‚úì (Ver consola para precisi√≥n)\\n\")\n",
    "                        if hasattr(clasificador.modelo, 'classes_'): # Si el modelo se entren√≥\n",
    "                            prediccion = clasificador.predecir(df.head(1)) # Predecir sobre la primera fila\n",
    "                            probabilidades = clasificador.predecir_probabilidades(df.head(1))\n",
    "                            f.write(f\"   - Predicci√≥n de prueba (1ra muestra): {prediccion[0] if prediccion else 'N/A'}\\n\")\n",
    "                            f.write(f\"   - Probabilidad m√°xima (1ra muestra): {max(probabilidades[0]) if probabilidades and probabilidades[0].size > 0 else 'N/A':.3f}\\n\")\n",
    "                        else:\n",
    "                            f.write(\"   - Modelo no entrenado, no se pueden hacer predicciones.\\n\")\n",
    "                except Exception as e_clf:\n",
    "                    f.write(f\"   - ERROR en clasificaci√≥n: {str(e_clf)}\\n\")\n",
    "            else:\n",
    "                f.write(\"   - No hay suficientes datos (features o etiquetas) para la prueba de clasificaci√≥n.\\n\")\n",
    "\n",
    "        except FileNotFoundError as e_fnf:\n",
    "            f.write(f\"\\nERROR CR√çTICO: No se pudo cargar {e_fnf.filename}. Aseg√∫rate que los archivos base existan.\\n\")\n",
    "        except Exception as e_gen:\n",
    "            f.write(f\"\\nERROR GENERAL en el an√°lisis: {str(e_gen)}\\n\")\n",
    "        \n",
    "        f.write(\"\\n10. Recomendaciones (Colab):\\n\")\n",
    "        f.write(\"   - Aseg√∫rate de ejecutar las celdas en orden (instalaci√≥n, creaci√≥n de archivos, carga de datos, etc.)\\n\")\n",
    "        f.write(\"   - Verifica que 'configuracion/config.yaml' est√© correctamente definido en su celda.\\n\")\n",
    "        f.write(\"   - Sube tus archivos .txt a la carpeta 'entrada_bruta' o cr√©alos en una celda.\\n\")\n",
    "        f.write(\"   - Ejecuta la celda de 'extractor_features.py' para generar 'features_tfidf.csv' y 'etiquetas.csv'.\\n\")\n",
    "        f.write(\"   - Revisa los mensajes de 'Precisi√≥n en test' en la salida de la celda de entrenamiento del clasificador.\\n\")\n",
    "        \n",
    "        f.write(f\"\\n=== FIN DEL REPORTE (COLAB) ===\\n\")\n",
    "    \n",
    "    print(f\"Reporte generado en: {reporte_path}\")\n",
    "    return reporte_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Para ejecutar este script directamente en Colab (despu√©s de crear los otros archivos):\n",
    "    # Primero, aseg√∫rate que extractor_features.py haya creado los CSV necesarios.\n",
    "    # Puedes simular la ejecuci√≥n de extractor_features.py aqu√≠ si es necesario para la prueba:\n",
    "    print(\"Ejecutando extractor_features.py para asegurar datos...\")\n",
    "    exec(open('scripts/extractor_features.py').read())\n",
    "    print(\"Ejecutando clasificador.py para asegurar modelo...\")\n",
    "    exec(open('scripts/clasificador.py').read()) # Para que el modelo se entrene si es necesario\n",
    "    print(\"Ejecutando gestor_modelo_semantico.py para asegurar estructuras...\")\n",
    "    exec(open('scripts/gestor_modelo_semantico.py').read())\n",
    "\n",
    "    print(\"\\nIniciando an√°lisis del sistema...\")\n",
    "    analizar_sistema()\n",
    "    print(\"An√°lisis completado. Revisa el archivo 'reporte_analisis.txt' en la carpeta logs_sistema.\")\n",
    "\"\"\"\n",
    "with open('scripts/analizador_sistema.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(analizador_sistema_py_content)\n",
    "print(\"scripts/analizador_sistema.py creado.\")\n",
    "\n",
    "print(\"Todos los archivos .py y config.yaml han sido creados/escritos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e37bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Crear archivos de texto de entrada si no los subes manualmente\n",
    "textos_fenomenologicos = {\n",
    "    \"texto_fenomenologico_1.txt\": \"Primera experiencia sobre la percepci√≥n del espacio y la luz.\",\n",
    "    \"texto_fenomenologico_2.txt\": \"Recuerdo v√≠vido de una conversaci√≥n y las emociones asociadas.\",\n",
    "    \"texto_fenomenologico_3.txt\": \"La temporalidad se siente diferente en momentos de alta concentraci√≥n.\",\n",
    "    \"texto_fenomenologico_4.txt\": \"Mi cuerpo reacciona al fr√≠o de la ma√±ana.\",\n",
    "    \"texto_fenomenologico_5.txt\": \"Observo el mundo a trav√©s de la ventana, los objetos parecen lejanos.\",\n",
    "    \"texto_fenomenologico_6.txt\": \"La intersubjetividad se manifiesta en la mirada del otro.\",\n",
    "    \"texto_fenomenologico_7.txt\": \"Una reflexi√≥n sobre la intencionalidad de mis acciones pasadas.\",\n",
    "    \"texto_fenomenologico_8.txt\": \"La afectividad ti√±e todos mis recuerdos de la infancia.\"\n",
    "}\n",
    "\n",
    "for nombre_archivo, contenido_texto in textos_fenomenologicos.items():\n",
    "    with open(os.path.join('entrada_bruta', nombre_archivo), 'w', encoding='utf-8') as f:\n",
    "        f.write(contenido_texto)\n",
    "\n",
    "print(f\"{len(textos_fenomenologicos)} archivos de texto creados en 'entrada_bruta'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c831f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "import datetime # Asegurar que datetime est√© importado\n",
    "from typing import Dict, List # Para type hints si los usas en tus clases\n",
    "\n",
    "# Agregar el directorio scripts al path para que Python encuentre los m√≥dulos\n",
    "# Esto es crucial en Colab si tus scripts est√°n en un subdirectorio.\n",
    "if 'scripts' not in sys.path:\n",
    "    sys.path.append('scripts')\n",
    "if '.' not in sys.path: # Asegurar que el directorio actual tambi√©n est√© en el path\n",
    "    sys.path.append('.')\n",
    "\n",
    "# Ahora intenta importar tus m√≥dulos\n",
    "try:\n",
    "    from scripts.extractor_features import ExtractorFeatures, cargar_config_extractor\n",
    "    from scripts.clasificador import ClasificadorFenomenologico, cargar_config_clasificador\n",
    "    from scripts.gestor_modelo_semantico import GestorModeloSemantico, cargar_config_gestor\n",
    "    from scripts.analizador_sistema import analizar_sistema, cargar_config_analizador\n",
    "    # Si tienes clases en sistema_principal_v2.py que necesitas, imp√≥rtalas tambi√©n\n",
    "    # from sistema_principal_v2 import SistemaFenomenologicoV2 # Ejemplo\n",
    "    print(\"M√≥dulos importados correctamente.\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error al importar m√≥dulos: {e}\")\n",
    "    print(\"Aseg√∫rate que los archivos .py est√©n en el directorio 'scripts' y que la Celda 3 se haya ejecutado completamente.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error de importaci√≥n: {e}\")\n",
    "    print(\"Verifica las dependencias internas entre tus scripts.\")\n",
    "\n",
    "# Cargar configuraci√≥n (usando una de las funciones, son equivalentes)\n",
    "config_global = cargar_config_analizador() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faea8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ejecutando extractor_features.py para generar archivos CSV...\")\n",
    "# Usamos exec() para correr el script como si fuera desde la l√≠nea de comandos\n",
    "# Esto ejecutar√° el bloque if __name__ == \"__main__\": de extractor_features.py\n",
    "try:\n",
    "    # Leer el contenido del script\n",
    "    with open('scripts/extractor_features.py', 'r', encoding='utf-8') as f:\n",
    "        script_content = f.read()\n",
    "    # Ejecutar el script en el contexto global actual\n",
    "    exec(script_content, globals())\n",
    "    print(\"Extractor de features ejecutado.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: scripts/extractor_features.py no encontrado. Aseg√∫rate que la Celda 3 se ejecut√≥.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al ejecutar extractor_features.py: {e}\")\n",
    "\n",
    "# Verificar la creaci√≥n de archivos\n",
    "if os.path.exists('features_extraidas/features_tfidf.csv') and os.path.exists('features_extraidas/etiquetas.csv'):\n",
    "    print(\"Archivos features_tfidf.csv y etiquetas.csv generados correctamente.\")\n",
    "    df_check = pd.read_csv('features_extraidas/features_tfidf.csv')\n",
    "    et_check = pd.read_csv('features_extraidas/etiquetas.csv')\n",
    "    print(f\"features_tfidf.csv tiene {df_check.shape[0]} filas y {df_check.shape[1]} columnas.\")\n",
    "    print(f\"etiquetas.csv tiene {et_check.shape[0]} filas.\")\n",
    "else:\n",
    "    print(\"Error: Los archivos features_tfidf.csv o etiquetas.csv no fueron generados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f49fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEjecutando clasificador.py para entrenar el modelo...\")\n",
    "try:\n",
    "    with open('scripts/clasificador.py', 'r', encoding='utf-8') as f:\n",
    "        script_content_clf = f.read()\n",
    "    exec(script_content_clf, globals())\n",
    "    print(\"Clasificador ejecutado y modelo entrenado (si hab√≠a datos suficientes).\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: scripts/clasificador.py no encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al ejecutar clasificador.py: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEjecutando gestor_modelo_semantico.py para inicializar estructuras...\")\n",
    "try:\n",
    "    with open('scripts/gestor_modelo_semantico.py', 'r', encoding='utf-8') as f:\n",
    "        script_content_gms = f.read()\n",
    "    exec(script_content_gms, globals())\n",
    "    print(\"Gestor del modelo sem√°ntico ejecutado.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: scripts/gestor_modelo_semantico.py no encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al ejecutar gestor_modelo_semantico.py: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44695bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEjecutando el an√°lisis completo del sistema...\")\n",
    "try:\n",
    "    # La funci√≥n analizar_sistema ya est√° importada\n",
    "    ruta_reporte = analizar_sistema() # Llama a la funci√≥n directamente\n",
    "    print(f\"An√°lisis del sistema completado. Reporte en: {ruta_reporte}\")\n",
    "    \n",
    "    # Mostrar el contenido del reporte\n",
    "    if os.path.exists(ruta_reporte):\n",
    "        with open(ruta_reporte, 'r', encoding='utf-8') as f_report:\n",
    "            print(\"\\n--- Contenido del Reporte ---\")\n",
    "            print(f_report.read())\n",
    "            print(\"--- Fin del Reporte ---\")\n",
    "    else:\n",
    "        print(f\"No se pudo encontrar el archivo de reporte en {ruta_reporte}\")\n",
    "        \n",
    "except NameError as ne:\n",
    "    print(f\"Error de nombre: {ne}. Aseg√∫rate que la funci√≥n 'analizar_sistema' est√© correctamente importada y definida.\")\n",
    "    print(\"Verifica la Celda 5 (Importaciones).\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurri√≥ un error durante el an√°lisis del sistema: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
